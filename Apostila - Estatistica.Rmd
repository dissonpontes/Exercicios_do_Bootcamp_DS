---
title: "Apostila IGTI - An√°lise Estat√≠stica de Dados"
subtitle: "Escrito por M√°iron C√©sar Sim√µes Chaves. Adaptado por Anderson Pontes"
output: github_document
---

# Cap√≠tulo 01 - An√°lise Explorat√≥ria de Dados com o R

Criando o **dataframe** que possui os dados que ser√£o explorados.
  atribuimos a uma vari√°vel 'dados' o data.frame que cont√©m quatro colunas, do tipo _factor_: Vendas_Cafe, Preco_Cafe, Promocao, Preco_Leite.
  tr√™s dessas colunas possuem dados num√©ricos (Vendas_Cafe, Preco_Cafe, Preco_Leite) e uma delas possui dados categ√≥ricos (Promocao)

```{r}
dados <- data.frame(Vendas_Cafe = c(18, 20, 23, 23, 23, 23, 24, 25, 26, 26, 26, 26, 27, 28, 28, 29, 29, 30, 30, 31, 31, 33, 34, 35, 38, 39, 41, 44, 44, 46),
 Preco_Cafe = c(4.77, 4.67, 4.75, 4.74, 4.63, 4.56, 4.59, 4.75, 4.75, 4.49, 4.41, 4.32, 4.68, 4.66, 4.42, 4.71, 4.66, 4.46, 4.36, 4.47, 4.43, 4.4, 4.61, 4.09, 3.73, 3.89, 4.35, 3.84, 3.81, 3.79),
 Promocao = c("Nao", "Nao", "Nao", "Nao", "Nao", "Nao", "Nao", "Nao", "Sim", "Nao", "Sim", "Nao", "Nao", "Sim", "Sim", "Nao", "Sim", "Sim", "Sim", "Nao", "Nao", "Sim", "Sim", "Sim", "Nao", "Sim","Sim", "Sim", "Sim", "Sim"),
 Preco_Leite = c(4.74, 4.81, 4.36, 4.29, 4.17, 4.66, 4.73, 4.11, 4.21, 4.25, 4.62, 4.53, 4.44, 4.19, 4.37, 4.29, 4.57, 4.21, 4.77, 4, 4.31, 4.34, 4.05, 4.73, 4.07, 4.75, 4, 4.15, 4.34, 4.15))
```

Agora, vamos visualizar um conjunto estat√≠sticas descritivas com a fun√ß√£o **summary()**, que retorna:
  para dados num√©ricos: valor m√≠nimo e m√°ximo, quartis e m√©dia.
  para dados categ√≥ricos: tipo de dados e quantidade de dados (length) 

```{r}
summary(dados)
```

Determinando o *desvio padr√£o* de cada uma das colunas num√©ricas, com a fun√ß√£o *sd()*

```{r}
sd(dados$Vendas_Cafe)
sd(dados$Preco_Cafe)
sd(dados$Preco_Leite)
```

Visualizaremos em um *histograma* a distribui√ß√£o da vari√°vel Pre√ßo_cafe, com a fun√ß√£o **hist()**, que possui como par√¢metros:
  col = determina a cor das colunas
  main = determina o t√≠tulo do gr√°fico
  xlab, ylab = determina o t√≠tulo dos eixos dos gr√°ficos

```{r}
hist(dados$Preco_Cafe, col = 'blue',  main = 'Distribuicao dos Pre√ßos Praticados para o Caf√©', xlab='Pre√ßos do caf√©', ylab='Frequ√™ncia')
```

Agora, visualizaremos o histograma das tr√™s vari√°veis num√©ricas na mesma p√°gina

```{r}
par(mfrow=c(2,2)) #Configura layout para posicionar os graficos em duas linhas e duas colunas
hist(dados$Vendas_Cafe, col = 'black', main = 'Distribuicao das Vendas do Caf√©', xlab="Vendas de caf√©", ylab="Frequ√™ncia")
hist(dados$Preco_Cafe, col = 'blue', main = 'Distribuicao dos Pre√ßos do Caf√©', xlab="Pre√ßos do caf√©", ylab="Frequ√™ncia")
hist(dados$Preco_Leite, col = 'red', main = 'Distribuicao dos Pre√ßos do Leite', xlab="Pre√ßos do leite", ylab="Frequ√™ncia")
```

Visualiza relacao entre as vendas do caf√© o pre√ßo do caf√©

```{r}
plot(y = dados$Vendas_Cafe,  x = dados$Preco_Cafe)
```

Customizando o gr√°fico anterior, temos:

```{r}
plot(y = dados$Vendas_Cafe,
 x = dados$Preco_Cafe,
 pch = 16,
 col = 'blue',
 xlab = 'Pre√ßo',
 ylab = 'Quantidade Vendidade',
 main = 'Rela√ß√£o entre o Pre√ßo e as Vendas do Caf√©')
grid() #este comando adiciona linhas de grade ao grafico

```

Agora, iremos colorir os pontos onde havia promo√ß√£o naquele dia, utilizando os dados categ√≥ricos da coluna Promocao para decidir.

```{r}
plot(y = dados$Vendas_Cafe,
 x = dados$Preco_Cafe,
 col = factor(dados$Promocao), #aqui, temos que transformar os dados da coluna Promocao em um factor
 pch = 16,
 xlab = 'Pre√ßo',
 ylab = 'Quantidade Vendidade',
 main = 'Rela√ß√£o entre o Pre√ßo e as Vendas do Caf√©')

#o trecho abaixo adiciona uma legenda no gr√°fico. o comando grid() adiciona as linhas pontilhadas no gr√°fico

legend(x=4.4,y=45,
 c("Promo√ß√£o","Sem_Promo√ß√£o"),
 col=c("red","black"),
 pch=c(16,16))
grid()

```

Criaremos uma nova variavel informando se naquele dia vendeu acima ou abaixo da media historica.

```{r}
media <- mean(dados$Vendas_Cafe) #armazena a media em uma variavel
variavel <- ifelse(dados$Vendas_Cafe > media, 'Acima_da_media', 'Abaixo_da_media')
variavel <- factor(variavel) #converte nova variavel para factor
plot(variavel) #grafico com a qtde abaixo e acima da media
table(variavel) #visualiza a qtde abaixo e acima da media
```

Geraremos os boxplots das colunas num√©ricas

```{r}
par(mfrow=c(2,2)) #Configura layout para posicionar os graficos em duas linhas e duas colunas
#Gera boxplot das vendas
boxplot(dados$Vendas_Cafe, main="Vendas de caf√©")

#Gera boxplot do preco
boxplot(dados$Preco_Cafe, main="Pre√ßo do caf√©")


#Customizando o boxplot
boxplot(dados$Vendas_Cafe~dados$Promocao,
 col = 'gray', pch = 16,
 xlab = 'Promo√ß√£o',
 ylab = 'Vendas',
 main = 'Vendas com promo√ß√£o vs Vendas sem promo√ß√£o')

```
# Cap√≠tulo 02 - Distribui√ß√µes de Probabilidade

## DISTRIBUI√á√ÉO BINOMIAL

Exemplo: Definindo como sucesso o cliente comprar, e supondo que a probabilidade de sucesso √© 50%.Ao passar 10 clientes em nossa loja, qual a probabilidade de realizarmos 2 vendas? Ou seja, queremos encontrar a probabilidade de dois sucessos, em dez tentativas.Cuja probabilidade de sucesso em cada tentativa √© 50%

```{r}
dbinom (x = 2, size = 10, prob = 0.5)
```
Onde:
x √© o n√∫mero de sucessos, size √© o n√∫mero de tentativas, prob √© a probabilidade de sucesso em cada tentativa

A fun√ß√£o a seguir gera quantidades aleat√≥rias de sucesso oriundos de uma quantidade (size) de tentativas dada a probabilidade (prob) de sucesso. √â √∫til para realizar experimentos. Podemos simular qual a frequencia esperada de vendas a cada dez clientes? Ainda mantendo a probabilidade de sucesso (cliente comprar) de 50%.

```{r}
va_binomial <- rbinom(n = 30, size=10, prob=0.5) 
```

Onde:
n √© a quantidade de vezes que o experimento deve ser repetido, size √© o n√∫mero de tentativas a cada experimento, prob √© o n√∫mero de sucesso em cada uma das tentativas

```{r}
hist(va_binomial)
```

A maior barra no histograma representa a quantidade esperada de vendas. 

Ajuste o parametro n para 1000 e plote o histograma e observe como a distribui√ß√£o binomial se aproxima da normal. 

```{r}
va_binomial <- rbinom(n = 1000, size=10, prob=0.5)
hist(va_binomial)
```

Podemos tamb√©m querer a probabilidade de que at√© dois clientes comprem, ao inv√©s de saber a probabilidade de exatos dois comprarem. A probabilidade de at√© dois clientes comprarem √©:(probabilidade de nenhum cliente comprar) + (probabilidade de um cliente comprar) + (probabilidade de dois cliente comprarem).
Formalizando: P(X<=2) = P(X=0) + P(X=1) + P(X=2)

```{r}
pbinom(q = 2,size = 10, prob = 0.5)
```

A probabilidade de que at√© dois clientes comprem ao entrarem dez clientes, √© de 5,48%

## DISTRIBUI√á√ÉO GEOM√âTRICA

Exemplo: Definindo como sucesso o cliente comprar, e supondo que a probabilidade de sucesso √© 50%.Qual a probabilidade da primeira venda ocorrer quando o quinto cliente entrar na
loja?

```{r}
dgeom(x = 5, prob = 0.5)
```
Onde:
x √© o n√∫mero de tentativas
prob √© a probabilidade de sucessos

Podemos utilizar a mesma fun√ß√£o para nos dar a probabilidade do sucesso ocorrer na primeira tentativa, segunda tentativa, terceira tentativa ... at√© a d√©cima tentativa.

```{r}
va_geometrica <- dgeom(x = 1:10, prob = 0.5)
va_geometrica
plot(va_geometrica)
```

Veja como as probabilidades v√£o diminuindo. A probabilidade de sucesso de 50% √© relativamente alta, ent√£o √© muito provavel que o sucesso ocorra logo nas primeiras tentativas.

Podemos utilizar a distribui√ß√£o geom√©trica acumulada para saber qual a probabilidade do primeiro sucesso ocorrer na primeira tentativa OU na segunda tentativa OU na terceira tentativa. Formalizando, queremos: P(X<=3)

```{r}
va_geometrica_acumulada <- pgeom(0:3, prob = 0.5)
plot(va_geometrica_acumulada)
```

## DISTRIBUI√á√ÉO BINOMIAL NEGATIVA

Definindo como sucesso o cliente comprar, e supondo que a probabilidade de sucesso √© 50%.Qual a probabilidade de ter que entrar 8 clientes at√© que a segunda venda ocorra?

```{r}
dnbinom(x=2, size = 8, prob = 0.50) 
```

Onde:
x √© o n√∫mero de sucessos, size √© a quantidade de tentativas, prob √© a probabilidade de sucesso

## DISTRIBUI√á√ÉO POISSON

Exemplo: Uma loja recebe em m√©dia, 6 (ùù∫) clientes por minuto. Qual a probabilidade de que 5(x) clientes entrem em um minuto?

```{r}
dpois(x= 5,lambda = 6)
```

#Onde:
x √© a quantidade a ser testada, lambda √© a taxa m√©dia de ocorr√™cia do evento em um determinado per√≠odo de intervalo de tempo ou espa√ßo.

Podemos utilizar a mesma funcao para obter a probabilidade de entrar um cliente, dois clientes... quinze clientes

```{r}
va_poison <- dpois(x = 1:15, lambda = 6)
plot(va_poison)
```

Observe que os valores se distribuiem simetricamente en tormo de seis, isso acontece porque o par√¢metro lambda √© a m√©dia (e tamb√©m o desvio padr√£o) da distribui√ß√£o de Poisson.

Tamb√©m podemos obter a probabilidade acumulada de at√© 5 clientes entrarem na loja em um minuto. Formalizando, queremos: P(X<=5)

```{r}
va_poison <- ppois(1:5, lambda = 6)
plot(va_poison)
```

## DITRIBUI√á√ÉO NORMAL

Exemplo: Suponha que a distribui√ß√£o dos sal√°rios dos funcion√°rios de uma empresa sigam uma distribui√ß√£o normal com m√©dia ùúá = 2.500 e desvio padr√£o œÉ = 170. Ao selecionar aleatoriamente um indiv√≠duo dessa popula√ß√£o, qual a probabilidade de ter sal√°rio entre 2.400 e 2.600? Precisamos achar a probabilidade do indiv√≠duo ter um sal√°rio de at√© 2.600 e subtrair pela probabilidade do indiv√≠duo ter o sal√°rio at√© 2.400.

Para P(X<=2400):

```{r}
probabilidade_ate_2400 <- pnorm(q = 2400, mean = 2500, sd = 170)
probabilidade_ate_2400
```
e para P(X<=2600)

```{r}
probabilidade_ate_2600 <- pnorm(q = 2600, mean = 2500, sd =170 )
probabilidade_ate_2600
```

determinando o intervalo P(X<=2600) - P(X<=2400), temos:

```{r}
probabilidade_ate_2600 - probabilidade_ate_2400
```

Podemos gerar 100 n√∫meros aleat√≥rios para uma distribui√ß√£o normal com m√©dia 2500 e desvio padr√£o 170

```{r}
va_normal <- rnorm(n = 100, mean = 2500,sd = 170)
hist(va_normal)
```

## DISTRIBUI√á√ÉO NORMAL PADR√ÉO

O comando scale() padroniza uma vari√°vel aleat√≥ria. Ao aplicar o comando na vari√°vel va_normal que acabmos de criar, ela ficar√° com m√©dia zero e desvio padr√£o unit√°rio

```{r}
va_normal_padrao <- scale(va_normal)
hist(va_normal_padrao)
```

Exemplo: Suponha que a distribui√ß√£o dos sal√°rios dos funcion√°rios de uma empresa sigam uma distribui√ß√£o normal com m√©dia ùúá = 2.500 e desvio padr√£o œÉ = 170. Ao selecionar aleatoriamente um indiv√≠duo dessa popula√ß√£o, qual a probabilidade de ter sal√°rio acima de 2.600?

Padroniza√ß√£o:

```{r}
z <- (2600-2500)/170
pnorm(z, mean = 0, sd = 1)
```

ou simplesmente 

```{r}
pnorm(z)
```

Podemos tamb√©m visualizar onde est√° o nosso valor Z em rela√ß√£o a m√©dia

```{r}
plot(density(scale(va_normal))) #Plota curva de densidade
abline(v = 0,col = 'blue') #Gera uma linha sobre m√©dia, que √© zero pois padronizamos a distribui√ß√£o
abline(v = 0.58,col = 'red') #Gera uma linha sobre o valor z obtido
```

## DISTRIBUI√á√ÉO F 

Gerando uma amostra aleat√≥ria de 1000 n√∫mero seguindo uma distribui√ß√£o F

```{r}
va_f <- rf( n= 1000, df1 = 5 , df2 = 33 ) 
```

Onde: n √© a quantidade de n√∫meros a ser gerado, df1 √© o primeiro grau de liberidade, df2 √© o segundo grau de liberdade.

Graficamente, temos:

```{r}
hist(va_f)
```

V√° aumentando os graus de liberdade e observe como a distribui√ß√£o se aproxima da normal

```{r}
va_f <- rf( n= 1000, df1 = 30 , df2 = 66 )
hist(va_f)
```
aumentando mais ainda

```{r}
va_f <- rf( n= 1000, df1 = 60 , df2 = 120 )
hist(va_f)
```

Informa√ß√£o Extra: Uma distribui√ß√£o F √© a raz√£o entre duas chi-quadrado

## DISTRIBUI√á√ÉO T

Gera uma amostra aleat√≥ria de 1000 n√∫meros seguindo uma distribui√ß√£o T

```{r}
va_t <- rt(1000, df = 2)
hist(va_t)
```

Observe que a distribui√ß√£o t, assim como a normal padr√£o, √© centrada no zero. 
V√° aumentando o grau de liberdade e observando o comportamento do histograma.

```{r}
va_t <- rt(1000, df = 10)
hist(va_t)
```
aumentando mais um pouco:

```{r}
va_t <- rt(1000, df = 50)
hist(va_t)
```

## DISTRIBUI√á√ÉO QUI-QUADRADO

Gera uma amostra aleat√≥ria de 1000 n√∫meros seguindo uma distribui√ß√£o quiquadrado

```{r}
va_QuiQuadrado <- rchisq(1000,df = 3)
hist(va_QuiQuadrado)
```

# Cap√≠tulo 03 - Intervalos de confian√ßa

##  Intervalo de confian√ßa para m√©dia amostral pela distribui√ß√£o Normal Padr√£o

Obter o intervalo de confian√ßa para uma vari√°vel cuja m√©dia = 30, desvio padr√£o = 7,31 e n = 30

Temos que definir o n√≠vel de confian√ßa do nosso intervalo. Podemos obter o valor do quantil para o n√≠vel de confian√ßa desejado com a fun√ß√£o qnorm().

O quantil na distribui√ß√£o normal padr√£o para 95% de confian√ßa:

```{r}
ic <- 0.95
alfa <- 1-ic
1-(alfa/2)
qnorm(0.975)
```
Vamos armazenar os valores em objetos

```{r}
media <- 30
desvio_padrao_populacional <- 7.31
n <- 30
quantil_95 <- qnorm(0.975)
```

Aplicando a f√≥rmula:

```{r}
Limite_Superior <- 30+quantil_95*(desvio_padrao_populacional/sqrt(n))
Limite_Inferior <- 30-quantil_95*(desvio_padrao_populacional/sqrt(n))
paste("Com 95% de confian√ßa, podemos afirmar que a m√©dia varia entre",Limite_Inferior," e ",Limite_Superior)
```

## Intervalo de confian√ßa para a m√©dia amostral pela distribui√ß√£o t de Student

A teoria nos diz para utilizar a distribui√ß√£o t de Student quando n√£o soubermos o desvio padr√£o populacional.

Vamos assumir que o desvio padr√£o que temos √© obtido a partir da amostra e armazenar os valores em objetos:

```{r}
media <- 30
desvio_padrao_amostral <- 7.31
n <- 30
quantil_95_t <- qt(0.975,df = n-1)
```

Aplicando a f√≥rmula:

```{r}
Limite_Superior_t <- 30+quantil_95_t*(desvio_padrao_amostral/sqrt(n))
Limite_Inferior_t <- 30-quantil_95_t*(desvio_padrao_amostral/sqrt(n))
paste("Com 95% de confian√ßa, podemos afirmar que a m√©dia varia entre",Limite_Inferior_t," e ",Limite_Superior_t)

```

Supondo que nossa vari√°vel j√° esteja em um data frame aqui no R, tem um comando para fornecer o intervalo de confian√ßa de forma bem mais f√°cil.

Vamos gerar com o comando rnorm() uma vari√°vel aleatoria com m√©dia 30, desvio padr√£o 7,31 e n = 30 e vizualizar seu padr√£o:

```{r}
va <- rnorm(n = 30, mean = 30, sd = 7.31)
hist(va)
```

Calculando o intervalo de 95% de confian√ßa com a distribui√ß√£o t de Student com a fun√ßao t.test():

```{r}
IC <-t.test(va, conf.level = 0.95)
IC$conf.int
```

Pronto, j√° temos o intervalo de confian√ßa para m√©dia. Beeem mais f√°cil assim :)

## Intervalo de confian√ßa para a propor√ß√£o

Utilizando o exemplo da apostila, onde calculamos o intervalo para propor√ß√£o onde 138 de n = 500 clientes realizaram a devolu√ß√£o do produto.

Vamos armazenar os valores em objetos:

```{r}
devolucoes <- 138
n <- 500
quantil_95 <-qnorm(0.975)
proporcao_devolucoes <- devolucoes/n
```

Aplicando a f√≥rmula, temos:

```{r}
Limite_Superior_prop <- proporcao_devolucoes + quantil_95 * sqrt(proporcao_devolucoes*(1-proporcao_devolucoes)/n)
Limite_Inferior_prop <- proporcao_devolucoes - quantil_95 * sqrt(proporcao_devolucoes*(1-proporcao_devolucoes)/n)
paste("Com 95% de confian√ßa, podemos afirmar que a propor√ß√£o varia entre",Limite_Inferior_prop," e ",Limite_Superior_prop)
```

Podemos obter o intervalo de confian√ßa para propor√ß√£o mais f√°cil pela fun√ß√£o prop.test()

```{r}
IC_proporcao <- prop.test(x = 138, n = 500, conf.level = 0.95)
IC_proporcao$conf.int
```

## Intervalo de confian√ßa para m√©dia via Bootstrap

Vamos gerar uma va seguindo uma distribui√ß√£o qui-quadrado

```{r}
va <- rchisq(n = 60, df = 3)
hist(va)
```

Inicializa variaveis

```{r}
medias <- c() #Essa vari√°vel √© um vetor para armazenar a m√©dia de cada subamostra bootstrap
R <- 1000 #Numero de subamostras extra√≠das para gerar a distribui√ß√£o amostral de m√©dias
```

Fazendo o bootstrap:

```{r}
for (i in 1:R) {
 #Realiza uma subamostragem aleat√≥ria com reposi√ß√£o da va
reamostra <- sample(va, size = 50, replace = T)
 #Armazena a m√©dia da subamostra
 medias[i] <- mean(reamostra)
}

```

Distribuicao das m√©dias das subamostras (distribui√ß√£o amostral da m√©dia da va)

```{r}
hist(medias)
```

Observe que mesmo a vari√°vel original n√£o seguindo uma distribui√ß√£o normal, o Teorema Central do Limite nos garante que a distribui√ß√£o das m√©dias ser√° normal se n √© suficientemente grande.

A partir das m√©dias geradas, precisamos achar dois valores, o que corta a cauda inferior e o que corta a cauda superior da distribui√ß√£o. Lembrando que ela √© sim√©trica

Caso o intervalo desejado seja de 95% de confian√ßa, temos que ordenar essa distribui√ß√£o do menor valor para o maior e achar o valor que deixar√° 2,5% dos dados para tr√°s
e o valor que deixar√° 97,5% para tr√°s:

```{r}
(1-0.95)/2
1-(1-0.95)/2
```

Visualize o intervalo de confian√ßa via bootstrap

```{r}
quantile(medias, probs = c(0.025,0.975))
```

Vamos realizar mais um experimento: Geraremos uma va com m√©dia = 30 e desvio padr√£o amostral =7.31 e n = 30

```{r}
va <- rnorm(n = 30, mean = 30, sd = 7.31)
```

Iremos calcular o intervalo de confian√ßa usando o Bootstrap e tamb√©m com a distribui√ß√£o # t de Student. Compararemos os resultados.

Inicializa variavel para armazenar as m√©dias de cada subamostra, e realiza o bootstrap:

```{r}
medias <- c()
R <- 10000
for (i in 1:R) {
 #Realiza uma subamostragem aleat√≥ria com reposi√ß√£o da va
 reamostra <- sample(va, size = 20, replace = T)
 #Armazena a m√©dia da subamostra
 medias[i] <- mean(reamostra)
}
#Distribuicao das m√©dias das subamostras (distribui√ß√£o amostral da m√©dia da va)
hist(medias)

```

Limites inferior e superior do intervalo pelo bootstrap

```{r}
quantile( medias, probs = c(0.025,0.975))
```

Limites inferior e superior do intervalo via t de Student

```{r}
IC<-t.test(va, conf.level = 0.95)
IC$conf.int
```

# Cap√≠tulo 04 - Testes de Hip√≥teses

## Avaliando a normalidade de uma vari√°vel aleat√≥ria 

Gera a vari√°vel aleat√≥ria que segue distribui√ß√£o normal com n = 70, m√©dia = 40 e desvio padr√£o = 8

```{r}
set.seed(10)
va_normal <- rnorm(n = 70, mean = 25, sd = 8)
```

Gera v.a. que segue uma distribui√ß√£o F (n√£o normal) com n = 15, 2 graus de liberdade no numerados e 10 graus de liberdade no denominador

```{r}
va_nao_normal <- rf(n =15, df1 =2, df2 = 10)
```

Visualize o histograma das vari√°veis geradas. Observe como os dados se distribuem em torno do valor m√©dio na va normal

```{r}
hist(va_normal)
```

Observe como os dados n√£o se distribuem em torno de um valor m√©dio exibindo padr√£o assim√©trico

```{r}
hist(va_nao_normal)
```

Visualize o QQ-Plot. Observe como os pontos de dados seguem a linha reta qq norm da va normal

```{r}
qqnorm(va_normal)
qqline(va_normal) #Este comando √© para adicionar a linha
```

Observe como os pontos de dados n√£o seguem a linha reta na va n√£o normal

```{r}
qqnorm(va_nao_normal)
qqline(va_nao_normal) #Este comando √© para adicionar a linha
```

Vamos aplicar o teste de hip√≥teses Shapiro Wilk. O teste funciona sob as hip√≥teses
  H0: A vari√°vel segue uma distribui√ß√£o normal
  H1: A vari√°vel n√£o segue uma distribui√ß√£o normal

Fixe um n√≠vel de signific√¢ncia alfa e analise o p valor (p-value) do Shapiro Wilk. 
Se o p-value for menor que alfa a hip√≥tese nula deve ser rejeitada

```{r}
shapiro.test(va_normal)
shapiro.test(va_nao_normal)
```
## Teste t para diferen√ßa de m√©dias (duas amostras independentes) 

Iremos simular o exemplo da apostila. Iremos testar se:
  H0: As vendas na posi√ß√£o A s√£o iguais as Vendas na Posi√ß√£o B
  H1: As vendas na posi√ß√£o A s√£o diferentes das vendas na posi√ß√£o B

```{r}
mu1 <- 150.1 #Armazena as m√©dia de vendas na posi√ß√£o A
mu2 <- 182.1 #Armazena as m√©dia de vendas na posi√ß√£o B

s1 <- 17 #Armazena o desvio padr√£o das vendas na posi√ß√£o A
s2 <- 19.2 #Armazena o desvio padr√£o das vendas na posi√ß√£o B

n1 <- 25 #Armazena a quantidade observa√ß√µes registradas para de vendas na posi√ß√£o A
n2 <- 30 #Armazena a quantidade observa√ß√µes registradas para de vendas na posi√ß√£o B
```

Calcula nossa estat√≠stica de teste. Que √© o t calculado:

```{r}
t <- (mu1 - mu2) / sqrt( s1^2/n1 + s2^2/n2)
t #Visualize o valor de t calculado
```

Calcula os graus de liberdade da estat√≠stica de teste

```{r}
gl <- (s1^2/n1 + s2^2/n2)^2 /( (s1^2/n1)^2 / (n1-1) + (s2^2/n2)^2 / (n2-1) )
gl #Visualize a quantidade de graus de liberdade
```

Obtem o quantil (t cr√≠tico) para uma distribui√ß√£o t com gl graus de liberdade. A um alfa de 5%

```{r}
quantil <- qt(0.975,df = gl)
quantil #Visualize o t cr√≠tico
```

Esse √© o aspecto de uma distribui√ß√£o t com n=53 observa√ß√µes e com n - 1 graus de liberdade

Observe onde est√£o os valores cr√≠ticos que acabamos de encontrar

```{r}
plot(density(rt(n = 53,df = gl)),xlim = c(-7,7)) 
abline(v = quantil,col = 'blue',lwd = 2)
abline(v = -quantil,col = 'blue',lwd = 2)
abline(v = t, col = 'red')# Observe como o tcalculado √© muito menor que o tcr√≠tico. Est√° na regi√£o de rejei√ß√£o
```

Obtendo o valor p

P(Tcalculado > Tcritico)

```{r}
2*pt(q = t, df = gl)
```

Agora vamos realizar o mesmo teste de hip√≥tese utilizando a fun√ß√£o nativa do R, **t.test()**

Observe no output desta fun√ß√£o, que ela j√° nos da tudo pronto, t calculado e valor p

```{r}
vendas_A <- rnorm(n= 25, mean = 150.1, sd = 17)
vendas_B <- rnorm(n = 30, mean = 182.1, sd = 19.2)
t.test(vendas_A,vendas_B, alternative = 'two.sided')
```

Esse √© o aspecto de uma distribui√ß√£o t com n observa√ß√µes e com n - 1 graus de liberdade

```{r}
n <- 5
plot(density(rt(n = n,df = n-1))) 
```

Altere o valor de n de 5 em 5 observe que a medida que os graus de liberdade aumenta a distribui√ß√£o se aproxima da normal. 
Como os valores s√£o geradosa leatoriamente poderemos ter curvas diferentes para um mesmo valor de n, mas a medida que n cresce o comportamento sim√©trico tende a estabilizar

```{r}
n <- 10
plot(density(rt(n = n,df = n-1))) 
```

```{r}
n <- 15
plot(density(rt(n = n,df = n-1))) 
```

## Teste t para diferen√ßa de m√©dias (duas amostras dependentes) 

Iremos simular o exemplo da apostila:
  H0: O peso m√©dio ap√≥s a dieta √© igual ao peso m√©dio antes da dieta
  H1: O peso m√©dio ap√≥s a dieta √© menor do que o peso m√©dio antes da dieta
  
Iremos utilizar uma biblioteca adicional para gerar valores aleat√≥rios que sigam uma distribui√ß√£o normal entre um intervalo de valor para simular os pesos

A biblioteca chama 'truncnorm'. Basta instalar com o comando abaixo **install.packages()**.

Uma vez instalada n√£o h√° mais necessidade de instalar novamente. Basta carregar com o comando **library()**

```{r}
install.packages('truncnorm')
library(truncnorm)
set.seed(100)
```

Gera uma amostra aleat√≥ria, seguindo uma distribui√ß√£o normal cujo valor m√≠nimo √© 100 e o valor m√°ximo √© 140.

O valor de n=20, m√©dia = 123 e desvio padr√£o 18 

Com essa v.a. iremos simular os pesos dos indiv√≠duos antes da dieta

```{r}
antes_da_dieta <- rtruncnorm(n=20, a=100, b=140, mean=123, sd=18)
```


Gera uma amostra aleat√≥ria, seguindo uma distribui√ß√£o normal cujo valor m√≠nimo √© 110 e o valor m√°ximo √© 130.

O valor de n=20, m√©dia = 110 e desvio padr√£o 28 

Com essa v.a. iremos simular os pesos dos indiv√≠duos ap√≥s a dieta

```{r}
depois_da_dieta <- rtruncnorm(n=20, a=110, b=130, mean=110, sd=28)
```

Calcula e visualiza a diferen√ßa depois da dieta e antes da dieta, para cada indiv√≠duo

```{r}
diferenca <- depois_da_dieta-antes_da_dieta
hist(diferenca)
```

Avalie a normalidade da distribui√ß√£o da diferen√ßa

```{r}
shapiro.test(diferenca)
```

Aplica test t com os seguintes argumentos

```{r}
t.test(depois_da_dieta,antes_da_dieta,
 paired = TRUE, #Pareado
 alternative = "less", #Unilateral a esquerda
 conf.level = 0.9 #90 porcento de confian√ßa
 ) 
```

O comando **t.test()** acima nos da tudo que precisamos para executar e concluir o teste. Mas a t√≠tulo de conhecimento, podemos realizar o teste passo a passo:

```{r}
#Calcula a m√©dia das diferen√ßas
media <- mean(diferenca)

#Desvio padr√£o das diferen√ßas
desvio_padrao <- sd(diferenca)

#Quantidade de indiv√≠duos
n <- 20

#Obtem o t calculado
t_calculado <- media / (desvio_padrao/sqrt(n))

#Obtem o valor p para o t calculado com n - 1 graus de liberdade.
pt(q = t_calculado, df = n-1) 
```

Podemos tamb√©m obter o t cr√≠tico para uma distribui√ß√£o t com 19 (n-1 = 20-1) graus de liberdade ao n√≠vel de confian√ßa de 90%


```{r}
tcr√≠tico_teste_t_pareado <- -qt(p = 0.9, df = 19) #Devido ao teste ser unilateral a esquerda a distribui√ß√£o t ser sim√©trica, nossa estat√≠stica de teste ser√° negativa
```

Observe que o t calculado √© maior que o t critico. Como estamos em um teste unilateral a esquerda o t calculado estar√° fora da regi√£o de rejei√ß√£o caso seja maior que o t cr√≠tico

```{r}
t_calculado < tcr√≠tico_teste_t_pareado
```

## Teste Qui-Quadrado para associa√ß√£o entre vari√°veis categ√≥ricas 

Iremos simular o exemplo da apostila
  H0: O fato do cliente estar ou n√£o com crian√ßa n√£o tem rela√ß√£o com o fato de comprar ou n√£o comprar
  H1: O fato do cliente estar ou n√£o com crian√ßa tem rela√ß√£o com fato de comprar ou n√£o comprar

Vamos gerar um data frame contendo os dados da pesquisa:

```{r}
dados <- data.frame(Cliente = c("Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto", "Adulto", "Adulto", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto", "Adulto", "Adulto", "Adulto",
 "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto", "Adulto_com_Crianca", "Adulto",
 "Adulto", "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto", "Adulto_com_Crianca", "Adulto", "Adulto", "Adulto",
 "Adulto","Adulto","Adulto","Adulto","Adulto","Adulto"),

  Comprou = c("N√£o_Comprou", "N√£o_Comprou", "N√£o_Comprou", "N√£o_Comprou",
 "N√£o_Comprou", "N√£o_Comprou", "Comprou", "Comprou", "Comprou",
 "Comprou", "Comprou", "Comprou", "Comprou", "Comprou", "Comprou",
 "Comprou", "Comprou", "Comprou", "Comprou", "Comprou", "Comprou","Comprou", "Comprou", "Comprou", "N√£o_Comprou", "N√£o_Comprou",
 "N√£o_Comprou", "N√£o_Comprou", "Comprou", "N√£o_Comprou", "Comprou",
 "Comprou", "N√£o_Comprou", "N√£o_Comprou", "N√£o_Comprou",
"N√£o_Comprou",
 "N√£o_Comprou", "Comprou", "Comprou", "N√£o_Comprou", "N√£o_Comprou",
 "N√£o_Comprou", "N√£o_Comprou", "N√£o_Comprou",
"Comprou","Comprou","Comprou","Comprou","Comprou","Comprou")
)
```

Visualizando o conjunto de dados:

```{r}
View(dados)
```

Gera tabela de contig√™ncia 2x2

```{r}
tabela <- table(dados$Cliente,dados$Comprou)
tabela
```
Visualiza a tabela num gr√°fico de barras

```{r}
barplot(tabela)
```

O valor cr√≠tico para uma distribui√ß√£o qui-quadrado com (linhas-1) * (colunas-1) = 1 grau de liberdade ao n√≠vel de confian√ßa de 95%

```{r}
qchisq(p=0.95,df = 1)
```

O valor p unilateral fica

```{r}
1-pchisq(q=10.728,df=1)
```

Mesmo que o n√≠vel de confian√ßa fosse 99%, ainda ter√≠amos evid√™ncias para rejeitar H0

Assim como fizemos no test t, podemos usar um comando direto no R para realizar o teste qui-quadrado **chisq.test()**

```{r}
teste<-chisq.test(tabela,correct = F)
teste
```

Visualiza valores observados. Que nada mais √© do que a tabela original

```{r}
teste$observed
```

Visualiza valores esperados

```{r}
teste$expected
```

## ANOVA (An√°lise de Vari√¢ncia)

Vamos utilizar o exemplo da apostila:
  H0: N√£o h√° diferen√ßa no valor m√©dio gasto com bebidas em nenhuma das popula√ß√µes
  H1: H√° diferen√ßa no valor m√©dio gasto com bebidas em pelo menos uma das popula√ß√µes
  
Geramos um data frame contendo os dados da pesquisa:

```{r}
dados_anova <- data.frame(Gastos = c(174.770021661909, 161.329206619394,
 153.679900850863, 163.790338797433, 141.363480335882,
175.351592994046, 185.793398289321, 184.720273514352, 163.400459287948,
170.202462740626, 150.8549565713, 167.583106239899, 140.190492201897,
157.440088617225, 171.596654773339, 138.885665257324, 147.942698809323,
9.87474262516482, 50.5645554670016, 14.2586307887884, 8.5061846804934,
25.0875496696788, 17.0661987504312, 41.3867417301938, 20.8113941426179,
60.1224674502026, 35.5154028285664, 23.7622285692359, 34.6086119259266,
30.4321086925016, 27.8188980544904, 37.4729772794009, 30.7229538650678,
48.0452539322412, 78.9197865324734, 42.4926762466659, 8.81227865272712,
39.5751781629677, 37.1329656327517, 15.8016718071775, 5.74735216885902,
38.684069121093, 30.9398891106907, 34.7370783113952, 13.2630510987537,
19.6212096123791, 16.716945267481, 24.4037922212213, 4.63398786180773,
32.9436217626275, 21.511905851158, 31.4997283634204, 26.6610570873775,
34.6304034101472, 16.2704826042681, 11.2323425300881, 18.023244405391,
15.4790632095655, 8.25633422881043, 27.9053307974433, 72.3298402892867,
4.7263338963663, 14.4153129255327, 41.2234268777169, 50.5684226296565,
19.8344282661234, 8.81306901471397, 19.5112436004646, 55.6251926080436,
16.7592556127806, 20.3176176298076, 31.2073058210955, 17.0613250010048,
47.8590627884627, 2.59778754862417, 35.9470130480825, 2.39404093355522,
9.38425601777391, 25.2455048267186, 16.1960287769175, 43.530118783298,
32.7250288712979, 5.43268078364765, 44.5365791890593, 32.9831443965413,
28.2104605365607, 3.18609515001209, 14.3698142789208, 39.9617218607622,
50.564581262513, 10.4634451365926, 36.4842442182048, 13.1330189654278,
8.93702642184252, 12.1501174131844, 22.2552757873296, 15.1407470062459,
11.7525513477354, 16.2990775324815, 24.4627568806115, 2.87916580644454,
44.5453919973285, 38.0393535792355, 32.1985589022666, 0.357075783631849,
22.0703974352325, 50.7486034030794, 18.604230207709, 5.83122133978906,
19.9252025339318, 6.8366108202567, 27.5834177510951, 41.9303025963975,
3.077799353254, 28.0507001837521, 33.0042729903, 50.7366690908169,
30.1697285113061, 6.53184416916073, 7.53469171526227, 5.49225229796712,
9.53198727121377, 6.59266645551752, 19.8423174628847, 0.781567028951091,
22.1605754480815, 5.90830712162365, 54.3457453874529, 33.3341495203441,
37.2034845899045
), Estado_Civil = c("solteiros", "solteiros", "solteiros", "solteiros",
 "solteiros", "solteiros", "solteiros", "solteiros", "solteiros",
 "solteiros", "solteiros", "solteiros", "solteiros", "solteiros",
 "solteiros", "solteiros", "solteiros", "Casados", "Casados", "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Divorciados", "Divorciados", "Divorciados", "Divorciados", "Divorciados",
 "Divorciados", "Divorciados", "Divorciados", "Divorciados", "Divorciados",
 "Divorciados", "Divorciados", "Divorciados", "Divorciados",
"Divorciados"))
```

Visualiza o conjunto de dados

```{r}
View(dados_anova)
```

Podemos utilizar os recursos de visualiza√ß√£o da biblioteca **ggplot2** para visualizar a distribui√ß√£o dos gastos nas popula√ß√µes

```{r}
install.packages("ggplot2")
library(ggplot2)
ggplot(data = dados_anova, aes(x = Gastos, fill = Estado_Civil)) +
 geom_density(alpha=0.4)+
 xlim(-50,300)
```

√â bastante comum tamb√©m analisarmos a variabilidade nas distintas popula√ß√µes com uso de boxplot

```{r}
boxplot(dados_anova$Gastos ~ dados_anova$Estado_Civil, ylab="Gastos", xlab="Estado Civil")
```

Com o comando **aov()**, o R gera a tabela da ANOVA completa

```{r}
anova <- aov(Gastos~ #Vari√°vel resposta
 Estado_Civil, #Fator que queremos testar se exerce influencia na vari√°vel resposta
 data = dados_anova)
```

Visualize a tabela da ANOVA. Observe o F calculado e o valor p ( Pr > F)

```{r}
summary(anova)
```

O valor p √© praticamente zero. Mesmo que nosso n√≠vel de confian√ßa fosse 99,9% ainda ter√≠amos evid√™ncias para rejeitar H0

## Cap√≠tulo 05 - Regress√£o Linear

Instala e carrega biblioteca para gerar a curva ROC

```{r}
#install.packages('pROC') #Instala
library("pROC") #Carrega
```

Monte o dataset

```{r}
dados <- data.frame(Prova_Logica = c(2, 2, 5, 5, 5, 2, 3, 2, 1, 4,  5, 8, 1, 1, 3, 4, 3, 2, 1, 1, 8, 8, 1, 2, 1, 5, 3, 3, 5, 4, 4, 1, 8, 3, 2, 3, 3, 2, 1, 1, 5, 4, 1, 5, 3, 1, 4, 6, 1, 1, 8, 1, 1, 5, 1, 5, 3, 1, 1, 8, 1, 1, 1, 1, 1, 2, 1, 5, 5, 4, 2, 1, 8, 4, 5, 1, 3, 3, 3, 5, 3, 1, 7, 1, 1, 2, 9, 5, 3, 1, 5, 1, 4, 2, 1, 4, 3, 3, 8, 1, 1, 8, 5, 1, 1, 1, 5, 8, 5, 1, 4, 2, 5, 4, 5, 3, 3, 5, 5, 5, 5, 5, 8, 5, 4, 9, 8, 1, 3, 4, 2, 5, 1, 4, 3, 5, 5, 5, 6, 4, 3, 5, 7, 1, 8, 5, 7, 3, 2, 3, 2, 5, 5, 5, 5, 4, 4, 8, 1, 1, 2, 5, 3, 2, 7, 4, 1, 1, 1, 4, 5, 1, 1, 8, 3, 6, 8, 3,1, 3, 3, 2, 8, 4, 1, 1, 1, 1, 1, 2, 3, 4, 6, 2, 3, 3, 4, 2, 1, 5, 2, 4, 3, 3, 1, 3, 3, 3, 1, 3, 5, 6, 1, 5, 1, 5, 4, 3, 1, 6, 1, 4, 9, 3, 3, 2, 1, 1, 4, 3, 1, 3, 1, 1, 3, 7, 8, 1, 3, 5, 6, 3, 6, 5, 8, 5, 1, 1, 4, 2, 1, 8, 7, 5, 1, 1, 1, 6, 5, 7, 3, 3, 5, 1, 3, 5, 1, 8, 8, 1, 2, 3, 3, 3, 3, 7, 1, 9, 8, 4, 1, 7, 1, 1, 1, 5, 1, 1, 5, 3, 5, 1, 3, 6, 2, 1, 3, 4, 5, 6, 1, 5, 1, 5, 1, 1, 5, 1, 1, 1, 5, 1, 3, 7, 1, 4, 3, 7, 1, 1, 5, 4, 1, 1, 3, 5, 4, 2, 1, 5, 1, 1, 1, 8, 8, 5, 1, 2, 1, 6, 8, 3, 1, 5, 1, 5, 1, 4, 4, 8, 1, 1, 1, 5, 1, 1, 5, 4, 6, 8, 1, 3, 1, 6, 1, 1, 1, 1, 1, 8, 1, 5, 3, 1, 4, 4, 7, 2, 3, 3, 5, 8, 3, 1, 4, 1, 5, 1, 7, 2, 6, 4, 1, 3, 1, 8, 5, 5, 5, 3, 1, 4, 5, 3, 6, 1, 3, 3, 5, 4, 5, 3, 1, 4, 5, 1, 3, 6, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 5, 4, 8, 1, 5, 6, 6, 4, 2, 5, 5, 6, 1, 2, 5, 6, 4, 2, 1, 2, 7, 2, 8, 1, 3, 1, 1, 1, 6, 1, 4, 3, 1, 5, 2, 1, 1, 5, 4, 3, 1, 3, 1, 1, 2, 4, 5, 4, 5, 4, 3, 7, 4, 1, 7, 1, 8, 5, 3, 3, 5, 1, 2, 1, 5, 4, 4, 4, 3, 6, 8, 8, 1, 1, 1, 8, 8, 1, 5, 1, 4, 5, 1, 4, 1, 4, 1, 5, 1, 4, 4, 1, 1, 2, 5, 1, 4, 4, 5, 4, 1, 1, 5, 3, 5, 4, 1, 1, 5, 3, 3, 1, 5, 5, 4, 5, 7, 2, 5, 9, 4, 6, 1, 1, 1, 1, 8, 7, 2, 3, 2, 9, 3, 1, 5, 1, 3, 5, 1, 4, 6, 3, 1, 5, 1, 3, 9, 1, 4,
 8, 8, 1, 3, 2, 3, 3, 1, 5, 1, 1, 3, 1, 5, 3, 4, 3, 4, 1, 4, 3, 5, 1, 5, 2, 5, 8, 9, 4, 7, 4, 8, 5, 7, 5, 3, 5, 6, 3, 1, 5, 6, 4, 3, 5, 1, 2, 4, 1, 1, 2, 9, 5, 1, 5, 1, 2, 1, 5, 1, 2, 3, 5, 1, 1, 5, 3, 9, 5, 6, 9, 6, 1, 1, 9, 1, 3, 5, 4, 8, 4, 2, 7, 1, 6, 3, 7, 8, 1, 5, 5, 6, 1, 4, 4, 3, 1, 5, 5, 8, 3, 3, 1, 9, 5, 5, 5, 5, 1, 9, 6, 3, 1, 1, 2, 4, 6, 5, 7, 6, 5, 1), 
 
 Redacao = c(1, 1, 1, 4, 3, 3, 5, 5, 9, 1, 1, 1, 1, 1, 4, 3, 3, 1, 1, 1, 1, 7, 1, 1, 8, 1, 1, 1, 3, 8, 3, 1, 5, 3, 3, 1, 2, 7, 1, 1, 1, 1, 1, 1, 7, 1, 1, 8, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 8, 5, 1, 1, 1, 1, 1, 2, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 4, 1, 8, 5, 1, 5, 8, 1, 1, 1, 5, 1, 1, 1, 2, 3, 3, 1, 3, 8, 1, 4, 6, 1, 1, 1, 1, 3, 1, 1, 2, 3, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 8, 1, 5, 1, 1, 1, 1, 1, 3, 6, 1, 2, 5, 6, 1, 2, 2, 1, 8, 1, 4, 6, 9, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 3, 5, 1, 1, 1, 1, 3, 1, 4, 1, 1, 1, 2, 1, 3, 1, 1, 1, 4, 5, 1, 1, 6, 1, 3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 6, 1, 8, 1, 1, 5, 1, 8, 2, 6, 1, 5, 1, 6, 1, 1, 1, 1, 1, 1, 1,1, 3, 1, 4, 8, 1, 1, 1, 8, 1, 3, 1, 6, 3, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 5, 1, 1, 3, 1, 1, 7, 4, 1, 1, 1, 1, 6, 1, 3,1, 4, 1, 1, 7, 2, 6, 4, 1, 1, 1, 1, 1, 4, 7, 1, 3, 1, 1, 9, 1,1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 3, 1, 1, 3, 2, 1, 1, 1, 5, 3, 1,1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 8, 1, 7, 1,1, 3, 8, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 2, 7, 1, 3, 1, 1, 1, 4,2, 4, 2, 2, 5, 3, 1, 1, 1, 5, 1, 9, 1, 1, 3, 2, 1, 1, 5, 1, 2,1, 3, 8, 1, 5, 1, 4, 3, 1, 8, 1, 6, 5, 1, 1, 1, 1, 1, 4, 5, 1,7, 8, 1, 4, 1, 1, 1, 1, 4, 1, 1, 2, 1, 8, 2, 6, 2, 1, 4, 1, 1,1, 1, 1, 4, 1, 1, 1, 1, 1, 8, 1, 1, 1, 3, 1, 1, 1, 8, 1, 1, 1,3, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1,3, 1, 2, 7, 2, 1, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 3, 1, 1, 5, 1,7, 1, 1, 1, 3, 6, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 6, 8, 8, 7, 2,1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 5, 1, 1, 1, 1, 9, 1, 8, 1, 1, 2,4, 1, 1, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 6, 1, 2,1, 1, 5, 4, 1, 8, 4, 6, 6, 1, 1, 1, 9, 1, 1, 1, 1, 1, 8, 1, 1,1, 1, 1, 3, 1, 1, 4, 1, 1, 3, 4, 1, 1, 3, 2, 3, 1, 2, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 4, 1, 4, 2, 1, 6, 1,
4, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, 6, 1, 1, 1, 3, 2, 8, 1, 1,1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 6, 1, 6, 7, 1, 1, 5, 1, 2, 5,1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 8, 7, 1, 1, 1, 1, 4, 1, 6, 1,2, 8, 4, 7, 1, 1, 1, 5, 1, 1, 2, 1, 1, 7, 1, 1, 1, 4, 1, 1, 3,1, 5, 1, 7, 1), 

Auto_Avaliacao = c(1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 9, 1, 1, 1, 3, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 9, 1, 1, 1, 1, 1, 1, 8, 1, 1, 9, 7, 1, 2,
 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 3, 8, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 8, 3, 1, 6, 1, 6, 1, 1, 3, 1, 1,1, 1, 8, 5, 3, 3, 1, 1, 3, 1, 1, 1, 1, 1, 6, 1, 2, 1, 1, 1, 1, 1, 4, 1, 6, 1, 1, 1, 2, 3, 1, 4, 7, 6, 1, 1, 1, 1, 1, 1, 9, 1, 2, 1, 1, 1, 1, 2, 1, 8, 1, 8, 1, 3, 2, 1, 1, 1, 1, 2, 6, 1, 1, 1, 2, 1, 3, 1, 1, 8, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 1, 5, 1, 1, 1, 1, 1, 5, 1, 1, 1, 3, 5, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 6, 1, 8, 1, 2, 5, 2, 1, 1, 7, 1, 1, 1, 8, 1, 1, 5, 1, 1, 1, 3, 1, 1, 1, 9, 1, 1, 1, 5, 9, 1, 5, 3, 4, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, 3, 8, 1, 4, 1, 4, 1, 1, 1, 6, 1, 3, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 9, 1, 1, 2, 2, 8, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 4, 3, 1, 1, 4, 1, 6, 2, 1, 5, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 8, 3, 4, 8, 1, 3, 7, 7, 1, 1, 2, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 9, 1, 1, 1, 1, 8, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 9, 6, 3, 3, 1, 2, 1, 8, 7, 1, 1, 1, 1, 1, 6, 3, 1, 4, 5, 1, 6, 1, 1, 1, 1, 3, 1, 1, 2, 1, 6, 3, 7, 1, 8, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 9, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 7, 1, 1, 4, 2, 1, 5, 1, 5, 1, 1, 1, 4, 6, 1, 1, 4, 1, 2, 1, 1, 1, 9, 8, 1, 9, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 3, 1, 1, 8, 1, 1, 6, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 5, 1, 6, 3, 1, 4, 3, 1, 7, 5, 1, 1, 1, 1, 1, 1, 2, 1, 5, 9, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 8, 1, 1, 2, 5, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 5, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 6, 1, 9, 5, 3, 1, 8, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 8, 1, 1, 1, 6, 1, 1, 1, 1, 9, 1, 1, 1),

Classe = c("Ruim",  "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim",  "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Ruim",  "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim",  "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa",  "Boa", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim",  "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim",  "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim",  "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim",  "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim",  "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim",  "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim",  "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim",  "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim",  "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa",  "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim",
 "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim",  "Boa", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa",  "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim","Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Boa", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim",
 "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim",
 "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Boa", "Boa", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Boa", "Ruim"))
```

Converte variavel resposta para factor

```{r}
dados$Classe <- factor(dados$Classe, levels = c('Ruim','Boa'))
```

Pequena analisa exploratoria, com a biblioteca **dplyr**

```{r}
#install.packages("dplyr")
library("dplyr")
dados %>% group_by(Classe) %>% summarise_all("mean")
```
Ajusta a regress√£o log√≠stica

```{r}
fit <- glm(Classe ~ Prova_Logica + Redacao + Auto_Avaliacao ,
 data = dados,
 family = binomial)
```

Visualiza resumo do modelo ajustado

```{r}
summary(fit)
```

Aplica exponenciacao nos coeficientes para interpretar

```{r}
exp(fit$coefficients)
```

Curva ROC

```{r}
prob = predict(fit, newdata = dados, type = "response")
roc = roc(dados$Classe ~ prob, plot = TRUE, print.auc = TRUE)
```

Obtem a predicao/probabilidade para cada observacao

```{r}
Probabilidade <- predict(fit, newdata= dados,type = 'response')
```

Se a probabilidade for maior que 50% classifica como 'Boa'

```{r}
Classe_Predita <- ifelse(Probabilidade > 0.5,"Boa","Ruim")
```

Visualiza data frame com as predicoes

```{r}
View(data.frame(dados,Probabilidade,Classe_Predita))
```

Gera matriz de confusao

```{r}
confusao <- table(Classe_Predita = Classe_Predita, Classe_Original = relevel(dados$Classe,ref = 'Boa'))
confusao
```

Armazena valores da matriz de confusao

```{r}
vp <- confusao[1,1];vp
fn <- confusao[2,1];fn
vn <- confusao[2,2];vn
fp <- confusao[1,2];fp
```

Calcula acuracia

```{r}
acuracia <- sum(diag(confusao))/ sum(confusao);acuracia
```

Calcula Sensitividade

```{r}
sensitividade <- vp /(vp+fn)
sensitividade
```

Cacula Especificidade

```{r}
especificidade <- vn / (vn + fp) 
especificidade
```

Analise de Sensitividade e Especificidade

(Todo o trecho de c√≥digo abaixo n√£o est√° funcionando perfeitamente e eu n√£o sei exatamente o porque. Est√° exatamente igual ao trecho usado pelo professor na video aula. Se algu√©m tiver uma luz, eu agrade√ßo!)

```{r}
limiares <- sort(Probabilidade)
acuracia <- c()
sensitividade <- c()
especificidade <- c()

for ( i in 1:length(limiares)) {
  limiar_atual <- limiares[i]
  Classe_Predita <- ifelse(Probabilidade > limiar_atual, 'Boa', 'Ruim')

#Gera matriz de confusao
  confusao <- table(Classe_Predita = Classe_Predita, Classe_Original = relevel(dados$Classe, ref = 'Boa'))
}
confusao
```

Armazena valores da matriz de confusao (sinceramente n√£o sei qual √© o erro nesse trecho. por causa dele, os pr√≥ximos trechos ficar√£o diferentes do que o professor utilizou em aula)

```{r}
vp <- confusao[1,1];vp
fn <- confusao[2,1];fn
vn <- confusao[2,2];vn
fp <- confusao[1,2];fp
```

Calcula acuracia

```{r}
acuracia[i] <- sum(diag(confusao))/ sum(confusao);
```

Calcula Sensitividade

```{r}
sensitividade[i] <- vp /(vp+fn)
```

Calcula Especificidade

```{r}
especificidade[i] <- vn / (vn + fp) 

```

Teoricamente, deveria mostrar o gr√°fico

```{r}
plot(y = sensitividade[1:698] , x = limiares[1:698], type="l", col="red", ylab =
'Sensitividade e Especificidade', xlab= 'Pontos de Corte')
grid()

lines(y = especificidade[1:698], x = limiares[1:698], type = 'l',col="blue" )
legend("bottomleft", c("sensibilidade","especificidade"),
 col=c("red","blue"), lty=c(1,1),bty="n", cex=1, lwd=1)
abline(v=0.225)
```

Obtem novamente as probabilidades para classificar baseado no ponto de corte 22,5%

```{r}
Probabilidade <- predict(fit, newdata= dados,type = 'response')
Classe_Predita <- ifelse(Probabilidade > 0.225,"Boa","Ruim")
View(data.frame(dados,Probabilidade,Classe_Predita))
```

Visualiza matriz de confusao final

```{r}
confusao <- table(Classe_Predita = Classe_Predita, Classe_Original =
relevel(dados$Classe,ref = 'Boa'))
```

Armazena valores da matriz de confusao

```{r}
vp <- confusao[1,1];vp
fn <- confusao[2,1];fn
vn <- confusao[2,2];vn
fp <- confusao[1,2];fp
```
Calculando acuracia, sensitividade e especificidade

```{r}
#Calcula acuracia
acuracia <- sum(diag(confusao))/ sum(confusao);acuracia
#Calcula Sensitividade
sensitividade <- vp /(vp+fn)
#Cacula Especificidade
especificidade <- vn / (vn + fp)
```

# Atividade Avaliativa do M√≥dulo 01

## Enunciado:

**Uma empresa que trabalha com aluguel de ve√≠culos deseja estudar algumas vari√°veis que eles suspeitam que influenciam no pre√ßo do aluguel do ve√≠culo. Para confirmar essas suspeitas, foi reunido em um conjunto dados um hist√≥rico contendo vinte loca√ß√µes que foram exportadas aleatoriamente do banco de dados, ou seja, temos vinte observa√ß√µes.**

Foram selecionadas sete vari√°veis para este estudo:

‚ñ™ **Pre√ßo** (vari√°vel cont√≠nua medida em reais) -- √â a vari√°vel resposta, nos diz qual foi o pre√ßo daquela loca√ß√£o.

‚ñ™ **Portas** (vari√°vel categ√≥rica com dois n√≠veis) -- Nos informa se o ve√≠culo alugado era de duas portas ou quatro portas.

‚ñ™ **Ar Condicionado** (vari√°vel categ√≥rica com dois n√≠veis) -- Nos informa se o ve√≠culo alugado tinha ar-condicionado ou n√£o.

‚ñ™ **Quadrimestre** (vari√°vel categ√≥rica com tr√™s n√≠veis) -- Informa se aquela loca√ß√£o ocorreu no primeiro, segundo ou terceiro quadrimestre do ano.

‚ñ™ **Idade do Locat√°rio** (vari√°vel discreta medida em anos) -- Nos informa qual a idade do indiv√≠duo que realizou a loca√ß√£o.

‚ñ™ **Quilometragem** (vari√°vel cont√≠nua medida em KM) -- Nos informa quantos KM rodados o ve√≠culo tinha no ato da loca√ß√£o.

‚ñ™ **D√≥lar** (vari√°vel cont√≠nua medida em d√≥lares) -- Nos informa qual a cota√ß√£o do d√≥lar no dia da loca√ß√£o.

Os alunos dever√£o desenvolver a pr√°tica e, depois, responder √†s quest√µes objetivas propostas.

### Cria√ß√£o do Data Frame a ser utilizado no processo

Forma o conjunto de dados historico contendo vinte locacoes sorteadas aleatoriamente do banco de dados e a armazena em um data frame chamado dados:

```{r}
dados <- data.frame(
Preco = c(368.384514890573, 446.850186825816, 
          414.72765691978, 434.291090918223, 436.652686535348, 457.65797344255, 
          490.694346597566, 474.881781399868, 458.462395897205, 412.719412673294, 
          448.799032112411, 352.040747235864, 449.461858221104, 416.150953927119, 
          416.499426750268, 551.315803331779, 462.126789471159, 515.957335395508, 
          467.598697162974, 339.548470369391), 
Portas = c("duas_portas", "quatro_portas", "duas_portas", "quatro_portas", "quatro_portas", 
          "duas_portas", "quatro_portas", "duas_portas", "quatro_portas", 
          "duas_portas", "quatro_portas", "quatro_portas", "duas_portas", 
          "quatro_portas", "duas_portas", "quatro_portas", "quatro_portas", 
          "duas_portas", "quatro_portas", "quatro_portas"),
Ar_Condicionado = c("sem_ar_condicionado",  "com_ar_condicionado", "com_ar_condicionado", "com_ar_condicionado", 
                  "com_ar_condicionado", "com_ar_condicionado", "com_ar_condicionado", 
                  "com_ar_condicionado", "com_ar_condicionado", "com_ar_condicionado", 
                  "com_ar_condicionado", "sem_ar_condicionado", "com_ar_condicionado", 
                  "com_ar_condicionado", "com_ar_condicionado", "com_ar_condicionado", 
                  "com_ar_condicionado", "com_ar_condicionado", "com_ar_condicionado", 
                  "sem_ar_condicionado"),
Quadrimestre = c("segundo_quadrimestre","segundo_quadrimestre", "segundo_quadrimestre", "segundo_quadrimestre", 
                 "segundo_quadrimestre", "terceiro_quadrimestre", "primeiro_quadrimestre", 
                 "primeiro_quadrimestre", "terceiro_quadrimestre", "segundo_quadrimestre", 
                 "terceiro_quadrimestre", "segundo_quadrimestre", "terceiro_quadrimestre", 
                 "segundo_quadrimestre", "segundo_quadrimestre", "primeiro_quadrimestre", 
                 "terceiro_quadrimestre", "primeiro_quadrimestre", "primeiro_quadrimestre", 
                 "segundo_quadrimestre"), 
Idade_Locatario = c(23, 18, 28, 21, 18, 21, 18, 20, 25, 29, 18, 33, 20, 21, 18, 21, 18, 20, 25, 29),
Quilometragem = c(957.442780544097, 829.533278217768, 923.300215829467, 871.519116905113, 930.704105677958, 554.696695914233, 501.941059782271, 
                  665.435074822519, 568.24079543466, 930.704105677958, 554.696695914233, 
                  829.533278217768, 665.435074822519, 871.519116905113, 930.704105677958, 
                  351.547138218644, 501.941059782271, 447.872006186523, 568.24079543466, 
                  930.704105677958), 
Dolar = c(4.41147933990862, 5.63014407874318, 
         8.80557934010615, 4.260591319988649, 6.93416279643155, 1.61130694543154, 
         2.57813244655973, 4.66666728709914, 1.6846066723224, 7.33872353619711, 
         4.52300814589177, 2.96689816205009, 9.91448182957733, 8.55577847959413, 
         5.93424935955983, 5.55775429484673, 6.94475470863839, 4.74330294976712, 
         4.723306965757987, 4.7010894862212))

View(dados)
```

### Pergunta 01: Explore a vari√°vel resposta PRE√áO e responda: Pelo histograma, voc√™ diria que a vari√°vel PRE√áO segue uma distribui√ß√£o normal?

a)  N√£o, pois o histograma apresenta comportamento assim√©trico a direita.
b)  N√£o, pois os dados s√£o distribu√≠dos simetricamente.
c)  N√£o, pois o histograma apresenta comportamento assim√©trico a esquerda.
d)  Sim, pela an√°lise gr√°fica a vari√°vel pre√ßo aparenta seguir uma distribui√ß√£o normal, pois os dados s√£o distribu√≠dos simetricamente em torno de um valor central.

```{r}
hist(dados$Preco)
```

### Pergunta 02: Explore a vari√°vel resposta, que √© o Pre√ßo, e responda: Pelo boxplot do Pre√ßo, voc√™ consegue visualizar algum outlier?

a)  N√£o h√° outlier, pois todos os valores est√£o entre a m√©dia e dois desvios padr√µes.
b)  N√£o h√° nenhum outlier, pois todos os valores est√£o entre o primeiro e o terceiro quartil.
c)  N√£o h√° nenhum outlier, pois todos os valores s√£o pr√≥ximos √† mediana.
d)  Sim, possui um outlier superior e um inferior, ou seja, existe uma loca√ß√£o que o pre√ßo foi muito acima do esperado e uma loca√ß√£o cujo pre√ßo foi muito abaixo do esperado. Antes de remover estes outliers de uma an√°lise, eles devem ser investigados sobre o que ocorreu naquelas loca√ß√µes que fez com o que o valor fosse t√£o discrepante da distribui√ß√£o da vari√°vel.

```{r}
boxplot(dados$Preco)
```

### Pergunta 03: Explore a vari√°vel resposta, que √© o Pre√ßo, e responda: Qual √© o valor mediano do Pre√ßo e qual a sua interpreta√ß√£o CORRETA?

a)  A mediana √© 447,8. Isso nos diz que 50% dos pre√ßos s√£o at√© este valor e os demais 50% s√£o acima deste valor.
b)  A mediana √© 447,8. Isso nos diz que 75% dos pre√ßos s√£o at√© este valor e os demais 25% s√£o acima deste valor.
c)  A mediana √© 447,8. Isso nos diz que 25% dos pre√ßos s√£o at√© este valor e os demais 75% s√£o acima deste valor.
d)  A mediana √© 447,8. Isso nos diz que o pre√ßo m√©dio √© 447,8

```{r}
summary(dados$Preco)
```

### Pergunta 04: Explore a rela√ß√£o entre as vari√°veis Pre√ßo e Quadrimestre, e responda: Atrav√©s do boxplot, como o Pre√ßo se comporta em rela√ß√£o a cada Quadrimestre?

a)  O segundo quadrimestre apresenta a maior mediana.
b)  A tr√™s medianas est√£o perfeitamente alinhadas e s√£o iguais.
c)  O primeiro quadrimestre apresenta a maior mediana.
d)  O terceiro quadrimestre apresenta a maior mediana.

```{r}
boxplot(dados$Preco~ dados$Quadrimestre)
```

### Pergunta 05: Explore a rela√ß√£o entre as vari√°veis Pre√ßo e Quadrimestre, e responda: Atrav√©s de uma ANOVA, existe diferen√ßa significativa entre o Pre√ßo m√©dio de pelo menos um Quadrimestre em rela√ß√£o aos outros? Como chegou a essa conclus√£o? Adote 95% de confian√ßa na sua interpreta√ß√£o.

a)  N√£o, pois o F Value √© maior que 0,05.
b)  Sim, considerando o alfa de 0,05 e ao p valor de 0,000126, h√° evid√™ncias para rejeitar a hip√≥tese nula de igualdade de m√©dias, ou seja, pelo menos um dos quadrimestres possui o pre√ßo m√©dio diferente dos demais.
c)  N√£o, pois os graus de liberdade residuais deram 17, ou seja, maior que 0,05.
d)  N√£o, pois a m√©dia quadr√°tica dos res√≠duos √© 1014, ou seja, maior que 0,05.

```{r}
anova <- aov(Preco ~ Quadrimestre, data = dados)
summary(anova)
```

### Pergunta 06: Explore a rela√ß√£o entre as vari√°veis Pre√ßo e Portas, e responda: Atrav√©s de um teste t de Student para amostras independentes, existe diferen√ßa significativa entre o pre√ßo m√©dio do aluguel do ve√≠culo com duas portas quando comparado com o pre√ßo m√©dio do ve√≠culo de quatro portas? Adote 95% de confian√ßa ao realizar na sua interpreta√ß√£o.

a)  Sim, pois as medianas n√£o s√£o iguais.
b)  Sim, pois os desvios padr√µes n√£o s√£o iguais.
c)  N√£o, pois ao p valor de 0,8884 n√£o h√° evid√™ncias para rejeitar a hip√≥tese nula de igualdade de m√©dias, ou seja, n√£o h√° diferen√ßa significativa entre o pre√ßo m√©dio de ve√≠culos de duas portas em rela√ß√£o ao pre√ßo m√©dio de ve√≠culos de quatro portas.
d)  Sim, pois o pre√ßo m√©dio dos ve√≠culos de duas portas √© 438,78 e o pre√ßo m√©dio dos ve√≠culos de quatro portas √© de 442,04.

```{r}
boxplot(dados$Preco ~ dados$Portas)
```

```{r}
#Test t de Student
t.test(dados$Preco ~ dados$Portas , 
       paired = FALSE, #amostras nao pareadas
       alternative = 'two.sided', #bilateral
       conf.level = 0.95 #95% de confianca
       )
```

### Pergunta 07: Explore a rela√ß√£o entre as vari√°veis Pre√ßo e Quilometragem, e responda: Pelo gr√°fico de dispers√£o, voc√™ identifica que existe rela√ß√£o linear entre o Pre√ßo e a Quilometragem? Se sim, a rela√ß√£o √© positiva ou negativa?

a)  Existe rela√ß√£o, mas √© uma rela√ß√£o quadr√°tica.
b)  Sim, a rela√ß√£o √© linear negativa, pois √† medida que a quilometragem aumenta o pre√ßo diminui.
c)  N√£o existe rela√ß√£o entre as duas vari√°veis.
d)  Sim, a rela√ß√£o √© positiva.

```{r}
plot(y = dados$Preco ,
     x = dados$Quilometragem,
     pch = 16)
```

### Pergunta 08: Explore a rela√ß√£o entre as vari√°veis Pre√ßo e Quilometragem, e responda: Obtenha o valor do coeficiente de correla√ß√£o linear de Pearson entre o Pre√ßo e a Quilometragem. Qual a interpreta√ß√£o CORRETA?

a)  O coeficiente de correla√ß√£o linear de Pearson √© -0,82, isso nos informa que √© uma correla√ß√£o negativa alta.
b)  O coeficiente de correla√ß√£o linear de Pearson √© -0,82, isso nos informa que √© uma correla√ß√£o negativa baixa.
c)  O coeficiente de correla√ß√£o linear de Pearson √© -0,82, isso nos informa que √© uma correla√ß√£o positiva alta.
d)  O coeficiente de correla√ß√£o linear de Pearson √© -0,82, isso nos informa que n√£o h√° correla√ß√£o.

```{r}
cor(dados$Preco, dados$Quilometragem)
```

### Pergunta 09: Explore a rela√ß√£o entre as vari√°veis Pre√ßo e Quilometragem, e responda: Se tentarmos utilizar somente a Quilometragem para prever o valor do Pre√ßo, o quanto da varia√ß√£o do Pre√ßo a vari√°vel Quilometragem consegue explicar? Em outas palavras, interprete o R2 da regress√£o linear do Pre√ßo em fun√ß√£o da Quilometragem.

a)  O R2 √© de 67,76%, ou seja, a vari√°vel Quilometragem consegue explicar 67,76% da varia√ß√£o do Pre√ßo.
b)  O R2 √© de 67,76%, ou seja, para cada quilometro aumentado, o Pre√ßo aumenta em m√©dia 67,76%.
c)  O R2 √© de 67,76%, ou seja, para cada quilometro aumentado, o Pre√ßo diminui em m√©dia 67,76%.
d)  O R2 √© de 67,76%, ou seja, para cada quilometro aumentado, o Pre√ßo aumenta em m√©dia 32,24% (1-0,6776).

```{r}
regressao_linear <- lm(Preco ~ Quilometragem, data = dados)
summary(regressao_linear)

```

### Pergunta 10: Explore a vari√°vel Quilometragem, e responda: Qual o valor do primeiro quartil e qual a sua interpreta√ß√£o CORRETA?

a)  O primeiro quartil √© 747,5, isso nos diz que at√© 25% dos ve√≠culos alugados possuem quilometragem at√© 554,7.
b)  O primeiro quartil √© 554,7, isso nos diz que at√© 50% dos ve√≠culos alugados possuem quilometragem at√© 554,7.
c)  O primeiro quartil √© 554,7, isso nos diz que at√© 75% dos ve√≠culos alugados possuem quilometragem at√© 554,7.
d)  O primeiro quartil √© 554,7, isso nos diz que at√© 25% dos ve√≠culos alugados possuem quilometragem at√© 554,7.

```{r}
summary(dados$Quilometragem)
```

### Pergunta 11: Explore a vari√°vel Quilometragem, e responda: Qual o valor do terceiro quartil e qual a sua interpreta√ß√£o CORRETA?

a)  O terceiro quartil √© 925,2, isso nos diz que at√© 50% dos ve√≠culos alugados possuem quilometragem at√© 925,2.
b)  O terceiro quartil √© 925,2, isso nos diz que at√© 25% dos ve√≠culos alugados possuem quilometragem at√© 925,2.
c)  O terceiro quartil √© 925,2, isso nos diz que at√© 75% dos ve√≠culos alugados possuem quilometragem at√© 925,2.
d)  O terceiro quartil √© 925,2, isso nos diz que at√© 75% dos ve√≠culos alugados possuem quilometragem at√© 719,3.

```{r}
summary(dados$Quilometragem)
```

### Pergunta 12: Explore a vari√°vel Quilometragem e responda: Qual √© o valor do coeficiente de varia√ß√£o e qual a sua interpreta√ß√£o CORRETA?

a)  O coeficiente de varia√ß√£o √© 27,74%, ou seja, os valores da vari√°vel quilometragem variam em m√©dia 27,74% em torno de sua m√©dia.
b)  O coeficiente de varia√ß√£o √© 27,74%, ou seja, os valores da vari√°vel quilometragem variam em m√©dia 27,74% em torno do terceiro quartil.
c)  O coeficiente de varia√ß√£o √© 27,74%, ou seja, os valores da vari√°vel quilometragem variam em m√©dia 27,74% em torno do desvio padr√£o.
d)  O coeficiente de varia√ß√£o √© 27,74%, ou seja, os valores da vari√°vel quilometragem variam em m√©dia 27,74% em torno do primeiro quartil.

```{r}
sd(dados$Quilometragem) / mean(dados$Quilometragem)
```

### Pergunta 13: Explore a correla√ß√£o entre o Dolar e o Preco, e responda: A correla√ß√£o entre as duas vari√°veis √© positiva ou negativa?

a)  A correla√ß√£o √© negativa.
b)  A correla√ß√£o √© positiva.
c)  Existe uma correla√ß√£o c√∫bica.
d)  O gr√°fico de dispers√£o n√£o apresenta nenhum padr√£o entre as duas vari√°veis, ou seja, na medida que o D√≥lar aumenta, o Pre√ßo n√£o cresce nem decresce, ou seja, n√£o h√° correla√ß√£o entre as duas vari√°veis.

```{r}
plot(y = dados$Preco,
     x = dados$Dolar,
     pch = 16)

```

### Pergunta 14: Explore a correla√ß√£o entre o Dolar e o Preco, e responda: Obtenha o coeficiente de correla√ß√£o linear de Pearson entre o Dolar e o Pre√ßo. Qual a interpreta√ß√£o CORRETA?

a)  O valor do coeficiente de correla√ß√£o linear de Pearson √© -0,06, o que indica correla√ß√£o positiva fraca.
b)  O valor do coeficiente de correla√ß√£o linear de Pearson √© -0,06, o que indica correla√ß√£o negativa forte.
c)  O valor do coeficiente de correla√ß√£o linear de Pearson √© -0,06, que indica aus√™ncia de correla√ß√£o linear.
d)  O valor do coeficiente de correla√ß√£o linear de Pearson √© -0,06, o que indica correla√ß√£o negativa moderada.

```{r}
cor(dados$Preco, dados$Dolar)
```

### Pergunta 15: Explore a correla√ß√£o entre o Dolar e o Preco, e responda: Se ajustarmos uma regress√£o linear entre o Dolar e o Preco, para tentar prever o Pre√ßo baseado no D√≥lar, seria poss√≠vel?

a)  Seria poss√≠vel, pois a estat√≠stica F √© menor que 5%.
b)  Seria poss√≠vel, pois o p valor do coeficiente beta do Dolar √© acima de 5%.
c)  Seria poss√≠vel, pois o valor do coeficiente beta √© negativo -1,573.
d)  N√£o seria poss√≠vel, pois o p valor do coeficiente beta do Dolar √© de 0,77 (77%), ou seja, independente do n√≠vel de signific√¢ncia adotado, a vari√°vel Dolar n√£o exerce influ√™ncia significativa na vari√°vel Preco, portanto, n√£o √© poss√≠vel prever o Pre√ßo baseado no D√≥lar.

```{r}
regressao_linear <- lm(Preco ~ Dolar, data = dados)
summary(regressao_linear)
```























