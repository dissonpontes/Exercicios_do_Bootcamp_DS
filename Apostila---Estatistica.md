Apostila IGTI - An√°lise Estat√≠stica de Dados
================

# √çndice

1.[Cap√≠tulo 01 - An√°lise Explorat√≥ria de Dados com o
R](#%20Cap√≠tulo%2001%20-%20An√°lise%20Explorat√≥ria%20de%20Dados%20com%20o%20R)
2.[Cap√≠tulo 02 - Distribui√ß√µes de
Probabilidade](#%20Cap√≠tulo%2002%20-%20Distribui√ß√µes%20de%20Probabilidade)
2.1 [Distribui√ß√£o Binomial](##%20DISTRIBUI√á√ÉO%20BINOMIAL) 2.2
[Distribui√ß√£o Geom√©trica](##%20DISTRIBUI√á√ÉO%20GEOM√âTRICA) 2.3
[Distribui√ß√£o Binomial
Negativa](##%20DISTRIBUI√á√ÉO%20BINOMIAL%20NEGATIVA) 2.4 [Distribui√ß√£o
Poisson](##%20DISTRIBUI√á√ÉO%20POISSON) 2.5 [Distribui√ß√£o
Normal](##%20DITRIBUI√á√ÉO%20NORMAL) 2.6 [Distribui√ß√£o Normal
Padr√£o](##%20DISTRIBUI√á√ÉO%20NORMAL%20PADR√ÉO) 2.7 [Distribui√ß√£o
F](##%20DISTRIBUI√á√ÉO%20F) 2.8 [Distribui√ß√£o T](##%20DISTRIBUI√á√ÉO%20T)
2.9 [Distribui√ß√£o Qui-Quadrado](##%20DISTRIBUI√á√ÉO%20QUI-QUADRADO)
3.[Cap√≠tulo 03 - Intervalos de
confian√ßa](#%20Cap√≠tulo%2003%20-%20Intervalos%20de%20confian√ßa) 3.1
[Intervalo de confian√ßa para m√©dia amostral pela distribui√ß√£o Normal
Padr√£o](##%20Intervalo%20de%20confian√ßa%20para%20m√©dia%20amostral%20pela%20distribui√ß√£o%20Normal%20Padr√£o)
3.2 [Intervalo de confian√ßa para a m√©dia amostral pela distribui√ß√£o t de
Student](##%20Intervalo%20de%20confian√ßa%20para%20a%20m√©dia%20amostral%20pela%20distribui√ß√£o%20t%20de%20Student)
3.3 [Intervalo de confian√ßa para a
propor√ß√£o](##%20Intervalo%20de%20confian√ßa%20para%20a%20propor√ß√£o) 3.4
[Intervalo de confian√ßa para m√©dia via
Bootstrap](##%20Intervalo%20de%20confian√ßa%20para%20m√©dia%20via%20Bootstrap)
4.[Cap√≠tulo 04 - Testes de
Hip√≥teses](#%20Cap√≠tulo%2004%20-%20Testes%20de%20Hip√≥teses) 4.1
[Avaliando a normalidade de uma vari√°vel
aleat√≥ria](##%20Avaliando%20a%20normalidade%20de%20uma%20vari√°vel%20aleat√≥ria)
4.2 [Teste t para diferen√ßa de m√©dias (duas amostras
independentes)](##%20Teste%20t%20para%20diferen√ßa%20de%20m√©dias%20(duas%20amostras%20independentes))
4.3 [Teste t para diferen√ßa de m√©dias (duas amostras
dependentes)](##%20Teste%20t%20para%20diferen√ßa%20de%20m√©dias%20(duas%20amostras%20dependentes))
4.4 [Teste Qui-Quadrado para associa√ß√£o entre vari√°veis
categ√≥ricas](##%20Teste%20Qui-Quadrado%20para%20associa√ß√£o%20entre%20vari√°veis%20categ√≥ricas)
4.5 [ANOVA (An√°lise de
Vari√¢ncia)](##%20ANOVA%20(An√°lise%20de%20Vari√¢ncia)) 5.[Cap√≠tulo 05 -
Regress√£o Linear](##%20Cap√≠tulo%2005%20-%20Regress√£o%20Linear) 6.
[Atividade Avaliativa do M√≥dulo
01](#%20Atividade%20Avaliativa%20do%20M√≥dulo%2001) 6.1 [Pergunta
01](###%20Pergunta%2001:%20Explore%20a%20vari√°vel%20resposta%20PRE√áO%20e%20responda:%20Pelo%20histograma,%20voc√™%20diria%20que%20a%20vari√°vel%20PRE√áO%20segue%20uma%20distribui√ß√£o%20normal?)
6.2 [Pergunta
02](###%20Pergunta%2002:%20Explore%20a%20vari√°vel%20resposta,%20que%20√©%20o%20Pre√ßo,%20e%20responda:%20Pelo%20boxplot%20do%20Pre√ßo,%20voc√™%20consegue%20visualizar%20algum%20outlier?)
6.3 [Pergunta
03](###%20Pergunta%2003:%20Explore%20a%20vari√°vel%20resposta,%20que%20√©%20o%20Pre√ßo,%20e%20responda:%20Qual%20√©%20o%20valor%20mediano%20do%20Pre√ßo%20e%20qual%20a%20sua%20interpreta√ß√£o%20CORRETA?)
6.4 [Pergunta
04](###%20Pergunta%2004:%20Explore%20a%20rela√ß√£o%20entre%20as%20vari√°veis%20Pre√ßo%20e%20Quadrimestre,%20e%20responda:%20Atrav√©s%20do%20boxplot,%20como%20o%20Pre√ßo%20se%20comporta%20em%20rela√ß√£o%20a%20cada%20Quadrimestre?)
6.5 [Pergunta
05](###%20Pergunta%2005:%20Explore%20a%20rela√ß√£o%20entre%20as%20vari√°veis%20Pre√ßo%20e%20Quadrimestre,%20e%20responda:%20Atrav√©s%20de%20uma%20ANOVA,%20existe%20diferen√ßa%20significativa%20entre%20o%20Pre√ßo%20m√©dio%20de%20pelo%20menos%20um%20Quadrimestre%20em%20rela√ß√£o%20aos%20outros?%20Como%20chegou%20a%20essa%20conclus√£o?%20Adote%2095%%20de%20confian√ßa%20na%20sua%20interpreta√ß√£o.)
6.6 [Pergunta
06](###%20Pergunta%2006:%20Explore%20a%20rela√ß√£o%20entre%20as%20vari√°veis%20Pre√ßo%20e%20Portas,%20e%20responda:%20Atrav√©s%20de%20um%20teste%20t%20de%20Student%20para%20amostras%20independentes,%20existe%20diferen√ßa%20significativa%20entre%20o%20pre√ßo%20m√©dio%20do%20aluguel%20do%20ve√≠culo%20com%20duas%20portas%20quando%20comparado%20com%20o%20pre√ßo%20m√©dio%20do%20ve√≠culo%20de%20quatro%20portas?%20Adote%2095%%20de%20confian√ßa%20ao%20realizar%20na%20sua%20interpreta√ß√£o.)
6.7 [Pergunta
07](###%20Pergunta%2007:%20Explore%20a%20rela√ß√£o%20entre%20as%20vari√°veis%20Pre√ßo%20e%20Quilometragem,%20e%20responda:%20Pelo%20gr√°fico%20de%20dispers√£o,%20voc√™%20identifica%20que%20existe%20rela√ß√£o%20linear%20entre%20o%20Pre√ßo%20e%20a%20Quilometragem?%20Se%20sim,%20a%20rela√ß√£o%20√©%20positiva%20ou%20negativa?)
6.8 [Pergunta
08](###%20Pergunta%2008:%20Explore%20a%20rela√ß√£o%20entre%20as%20vari√°veis%20Pre√ßo%20e%20Quilometragem,%20e%20responda:%20Obtenha%20o%20valor%20do%20coeficiente%20de%20correla√ß√£o%20linear%20de%20Pearson%20entre%20o%20Pre√ßo%20e%20a%20Quilometragem.%20Qual%20a%20interpreta√ß√£o%20CORRETA?)
6.9 [Pergunta
09](###%20Pergunta%2009:%20Explore%20a%20rela√ß√£o%20entre%20as%20vari√°veis%20Pre√ßo%20e%20Quilometragem,%20e%20responda:%20Se%20tentarmos%20utilizar%20somente%20a%20Quilometragem%20para%20prever%20o%20valor%20do%20Pre√ßo,%20o%20quanto%20da%20varia√ß√£o%20do%20Pre√ßo%20a%20vari√°vel%20Quilometragem%20consegue%20explicar?%20Em%20outas%20palavras,%20interprete%20o%20R2%20da%20regress√£o%20linear%20do%20Pre√ßo%20em%20fun√ß√£o%20da%20Quilometragem.)
6.10 [Pergunta
10](###%20Pergunta%2010:%20Explore%20a%20vari√°vel%20Quilometragem,%20e%20responda:%20Qual%20o%20valor%20do%20primeiro%20quartil%20e%20qual%20a%20sua%20interpreta√ß√£o%20CORRETA?)
6.11 [Pergunta
11](###%20Pergunta%2011:%20Explore%20a%20vari√°vel%20Quilometragem,%20e%20responda:%20Qual%20o%20valor%20do%20terceiro%20quartil%20e%20qual%20a%20sua%20interpreta√ß√£o%20CORRETA?)
6.12 [Pergunta
12](###%20Pergunta%2012:%20Explore%20a%20vari√°vel%20Quilometragem%20e%20responda:%20Qual%20√©%20o%20valor%20do%20coeficiente%20de%20varia√ß√£o%20e%20qual%20a%20sua%20interpreta√ß√£o%20CORRETA?)
6.13 [Pergunta
13](###%20Pergunta%2013:%20Explore%20a%20correla√ß√£o%20entre%20o%20Dolar%20e%20o%20Preco,%20e%20responda:%20A%20correla√ß√£o%20entre%20as%20duas%20vari√°veis%20√©%20positiva%20ou%20negativa?)
6.14 [Pergunta
14](###%20Pergunta%2014:%20Explore%20a%20correla√ß√£o%20entre%20o%20Dolar%20e%20o%20Preco,%20e%20responda:%20Obtenha%20o%20coeficiente%20de%20correla√ß√£o%20linear%20de%20Pearson%20entre%20o%20Dolar%20e%20o%20Pre√ßo.%20Qual%20a%20interpreta√ß√£o%20CORRETA?)
6.15 [Pergunta
15](###%20Pergunta%2015:%20Explore%20a%20correla√ß√£o%20entre%20o%20Dolar%20e%20o%20Preco,%20e%20responda:%20Se%20ajustarmos%20uma%20regress√£o%20linear%20entre%20o%20Dolar%20e%20o%20Preco,%20para%20tentar%20prever%20o%20Pre√ßo%20baseado%20no%20D√≥lar,%20seria%20poss√≠vel?)

# Cap√≠tulo 01 - An√°lise Explorat√≥ria de Dados com o R

Criando o **dataframe** que possui os dados que ser√£o explorados.
atribuimos a uma vari√°vel ‚Äòdados‚Äô o data.frame que cont√©m quatro
colunas, do tipo *factor*: Vendas\_Cafe, Preco\_Cafe, Promocao,
Preco\_Leite. tr√™s dessas colunas possuem dados num√©ricos (Vendas\_Cafe,
Preco\_Cafe, Preco\_Leite) e uma delas possui dados categ√≥ricos
(Promocao)

``` r
dados <- data.frame(Vendas_Cafe = c(18, 20, 23, 23, 23, 23, 24, 25, 26, 26, 26, 26, 27, 28, 28, 29, 29, 30, 30, 31, 31, 33, 34, 35, 38, 39, 41, 44, 44, 46),
 Preco_Cafe = c(4.77, 4.67, 4.75, 4.74, 4.63, 4.56, 4.59, 4.75, 4.75, 4.49, 4.41, 4.32, 4.68, 4.66, 4.42, 4.71, 4.66, 4.46, 4.36, 4.47, 4.43, 4.4, 4.61, 4.09, 3.73, 3.89, 4.35, 3.84, 3.81, 3.79),
 Promocao = c("Nao", "Nao", "Nao", "Nao", "Nao", "Nao", "Nao", "Nao", "Sim", "Nao", "Sim", "Nao", "Nao", "Sim", "Sim", "Nao", "Sim", "Sim", "Sim", "Nao", "Nao", "Sim", "Sim", "Sim", "Nao", "Sim","Sim", "Sim", "Sim", "Sim"),
 Preco_Leite = c(4.74, 4.81, 4.36, 4.29, 4.17, 4.66, 4.73, 4.11, 4.21, 4.25, 4.62, 4.53, 4.44, 4.19, 4.37, 4.29, 4.57, 4.21, 4.77, 4, 4.31, 4.34, 4.05, 4.73, 4.07, 4.75, 4, 4.15, 4.34, 4.15))
```

Agora, vamos visualizar um conjunto estat√≠sticas descritivas com a
fun√ß√£o **summary()**, que retorna: para dados num√©ricos: valor m√≠nimo e
m√°ximo, quartis e m√©dia. para dados categ√≥ricos: tipo de dados e
quantidade de dados (length)

``` r
summary(dados)
```

    ##   Vendas_Cafe      Preco_Cafe      Promocao          Preco_Leite   
    ##  Min.   :18.00   Min.   :3.730   Length:30          Min.   :4.000  
    ##  1st Qu.:25.25   1st Qu.:4.353   Class :character   1st Qu.:4.175  
    ##  Median :28.50   Median :4.480   Mode  :character   Median :4.325  
    ##  Mean   :30.00   Mean   :4.426                      Mean   :4.374  
    ##  3rd Qu.:33.75   3rd Qu.:4.668                      3rd Qu.:4.607  
    ##  Max.   :46.00   Max.   :4.770                      Max.   :4.810

Determinando o *desvio padr√£o* de cada uma das colunas num√©ricas, com a
fun√ß√£o *sd()*

``` r
sd(dados$Vendas_Cafe)
```

    ## [1] 7.310833

``` r
sd(dados$Preco_Cafe)
```

    ## [1] 0.3220568

``` r
sd(dados$Preco_Leite)
```

    ## [1] 0.2558082

Visualizaremos em um *histograma* a distribui√ß√£o da vari√°vel
Pre√ßo\_cafe, com a fun√ß√£o **hist()**, que possui como par√¢metros: col =
determina a cor das colunas main = determina o t√≠tulo do gr√°fico xlab,
ylab = determina o t√≠tulo dos eixos dos gr√°ficos

``` r
hist(dados$Preco_Cafe, col = 'blue',  main = 'Distribuicao dos Pre√ßos Praticados para o Caf√©', xlab='Pre√ßos do caf√©', ylab='Frequ√™ncia')
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

Agora, visualizaremos o histograma das tr√™s vari√°veis num√©ricas na mesma
p√°gina

``` r
par(mfrow=c(2,2)) #Configura layout para posicionar os graficos em duas linhas e duas colunas
hist(dados$Vendas_Cafe, col = 'black', main = 'Distribuicao das Vendas do Caf√©', xlab="Vendas de caf√©", ylab="Frequ√™ncia")
hist(dados$Preco_Cafe, col = 'blue', main = 'Distribuicao dos Pre√ßos do Caf√©', xlab="Pre√ßos do caf√©", ylab="Frequ√™ncia")
hist(dados$Preco_Leite, col = 'red', main = 'Distribuicao dos Pre√ßos do Leite', xlab="Pre√ßos do leite", ylab="Frequ√™ncia")
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

Visualiza relacao entre as vendas do caf√© o pre√ßo do caf√©

``` r
plot(y = dados$Vendas_Cafe,  x = dados$Preco_Cafe)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

Customizando o gr√°fico anterior, temos:

``` r
plot(y = dados$Vendas_Cafe,
 x = dados$Preco_Cafe,
 pch = 16,
 col = 'blue',
 xlab = 'Pre√ßo',
 ylab = 'Quantidade Vendidade',
 main = 'Rela√ß√£o entre o Pre√ßo e as Vendas do Caf√©')
grid() #este comando adiciona linhas de grade ao grafico
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

Agora, iremos colorir os pontos onde havia promo√ß√£o naquele dia,
utilizando os dados categ√≥ricos da coluna Promocao para decidir.

``` r
plot(y = dados$Vendas_Cafe,
 x = dados$Preco_Cafe,
 col = factor(dados$Promocao), #aqui, temos que transformar os dados da coluna Promocao em um factor
 pch = 16,
 xlab = 'Pre√ßo',
 ylab = 'Quantidade Vendidade',
 main = 'Rela√ß√£o entre o Pre√ßo e as Vendas do Caf√©')

#o trecho abaixo adiciona uma legenda no gr√°fico. o comando grid() adiciona as linhas pontilhadas no gr√°fico

legend(x=4.4,y=45,
 c("Promo√ß√£o","Sem_Promo√ß√£o"),
 col=c("red","black"),
 pch=c(16,16))
grid()
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

Criaremos uma nova variavel informando se naquele dia vendeu acima ou
abaixo da media historica.

``` r
media <- mean(dados$Vendas_Cafe) #armazena a media em uma variavel
variavel <- ifelse(dados$Vendas_Cafe > media, 'Acima_da_media', 'Abaixo_da_media')
variavel <- factor(variavel) #converte nova variavel para factor
plot(variavel) #grafico com a qtde abaixo e acima da media
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->

``` r
table(variavel) #visualiza a qtde abaixo e acima da media
```

    ## variavel
    ## Abaixo_da_media  Acima_da_media 
    ##              19              11

Geraremos os boxplots das colunas num√©ricas

``` r
par(mfrow=c(2,2)) #Configura layout para posicionar os graficos em duas linhas e duas colunas
#Gera boxplot das vendas
boxplot(dados$Vendas_Cafe, main="Vendas de caf√©")

#Gera boxplot do preco
boxplot(dados$Preco_Cafe, main="Pre√ßo do caf√©")


#Customizando o boxplot
boxplot(dados$Vendas_Cafe~dados$Promocao,
 col = 'gray', pch = 16,
 xlab = 'Promo√ß√£o',
 ylab = 'Vendas',
 main = 'Vendas com promo√ß√£o vs Vendas sem promo√ß√£o')
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->
\# Cap√≠tulo 02 - Distribui√ß√µes de Probabilidade

## DISTRIBUI√á√ÉO BINOMIAL

Exemplo: Definindo como sucesso o cliente comprar, e supondo que a
probabilidade de sucesso √© 50%.Ao passar 10 clientes em nossa loja, qual
a probabilidade de realizarmos 2 vendas? Ou seja, queremos encontrar a
probabilidade de dois sucessos, em dez tentativas.Cuja probabilidade de
sucesso em cada tentativa √© 50%

``` r
dbinom (x = 2, size = 10, prob = 0.5)
```

    ## [1] 0.04394531

Onde: x √© o n√∫mero de sucessos, size √© o n√∫mero de tentativas, prob √© a
probabilidade de sucesso em cada tentativa

A fun√ß√£o a seguir gera quantidades aleat√≥rias de sucesso oriundos de uma
quantidade (size) de tentativas dada a probabilidade (prob) de sucesso.
√â √∫til para realizar experimentos. Podemos simular qual a frequencia
esperada de vendas a cada dez clientes? Ainda mantendo a probabilidade
de sucesso (cliente comprar) de 50%.

``` r
va_binomial <- rbinom(n = 30, size=10, prob=0.5) 
```

Onde: n √© a quantidade de vezes que o experimento deve ser repetido,
size √© o n√∫mero de tentativas a cada experimento, prob √© o n√∫mero de
sucesso em cada uma das tentativas

``` r
hist(va_binomial)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->

A maior barra no histograma representa a quantidade esperada de vendas.

Ajuste o parametro n para 1000 e plote o histograma e observe como a
distribui√ß√£o binomial se aproxima da normal.

``` r
va_binomial <- rbinom(n = 1000, size=10, prob=0.5)
hist(va_binomial)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-14-1.png)<!-- -->

Podemos tamb√©m querer a probabilidade de que at√© dois clientes comprem,
ao inv√©s de saber a probabilidade de exatos dois comprarem. A
probabilidade de at√© dois clientes comprarem √©:(probabilidade de nenhum
cliente comprar) + (probabilidade de um cliente comprar) +
(probabilidade de dois cliente comprarem). Formalizando: P(X&lt;=2) =
P(X=0) + P(X=1) + P(X=2)

``` r
pbinom(q = 2,size = 10, prob = 0.5)
```

    ## [1] 0.0546875

A probabilidade de que at√© dois clientes comprem ao entrarem dez
clientes, √© de 5,48%

## DISTRIBUI√á√ÉO GEOM√âTRICA

Exemplo: Definindo como sucesso o cliente comprar, e supondo que a
probabilidade de sucesso √© 50%.Qual a probabilidade da primeira venda
ocorrer quando o quinto cliente entrar na loja?

``` r
dgeom(x = 5, prob = 0.5)
```

    ## [1] 0.015625

Onde: x √© o n√∫mero de tentativas prob √© a probabilidade de sucessos

Podemos utilizar a mesma fun√ß√£o para nos dar a probabilidade do sucesso
ocorrer na primeira tentativa, segunda tentativa, terceira tentativa ‚Ä¶
at√© a d√©cima tentativa.

``` r
va_geometrica <- dgeom(x = 1:10, prob = 0.5)
va_geometrica
```

    ##  [1] 0.2500000000 0.1250000000 0.0625000000 0.0312500000 0.0156250000
    ##  [6] 0.0078125000 0.0039062500 0.0019531250 0.0009765625 0.0004882812

``` r
plot(va_geometrica)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-17-1.png)<!-- -->

Veja como as probabilidades v√£o diminuindo. A probabilidade de sucesso
de 50% √© relativamente alta, ent√£o √© muito provavel que o sucesso ocorra
logo nas primeiras tentativas.

Podemos utilizar a distribui√ß√£o geom√©trica acumulada para saber qual a
probabilidade do primeiro sucesso ocorrer na primeira tentativa OU na
segunda tentativa OU na terceira tentativa. Formalizando, queremos:
P(X&lt;=3)

``` r
va_geometrica_acumulada <- pgeom(0:3, prob = 0.5)
plot(va_geometrica_acumulada)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-18-1.png)<!-- -->

## DISTRIBUI√á√ÉO BINOMIAL NEGATIVA

Definindo como sucesso o cliente comprar, e supondo que a probabilidade
de sucesso √© 50%.Qual a probabilidade de ter que entrar 8 clientes at√©
que a segunda venda ocorra?

``` r
dnbinom(x=2, size = 8, prob = 0.50) 
```

    ## [1] 0.03515625

Onde: x √© o n√∫mero de sucessos, size √© a quantidade de tentativas, prob
√© a probabilidade de sucesso

## DISTRIBUI√á√ÉO POISSON

Exemplo: Uma loja recebe em m√©dia, 6 (ùù∫) clientes por minuto. Qual a
probabilidade de que 5(x) clientes entrem em um minuto?

``` r
dpois(x= 5,lambda = 6)
```

    ## [1] 0.1606231

Onde: x √© a quantidade a ser testada, lambda √© a taxa m√©dia de ocorr√™cia
do evento em um determinado per√≠odo de intervalo de tempo ou espa√ßo.

Podemos utilizar a mesma funcao para obter a probabilidade de entrar um
cliente, dois clientes‚Ä¶ quinze clientes

``` r
va_poison <- dpois(x = 1:15, lambda = 6)
plot(va_poison)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-21-1.png)<!-- -->

Observe que os valores se distribuiem simetricamente en tormo de seis,
isso acontece porque o par√¢metro lambda √© a m√©dia (e tamb√©m o desvio
padr√£o) da distribui√ß√£o de Poisson.

Tamb√©m podemos obter a probabilidade acumulada de at√© 5 clientes
entrarem na loja em um minuto. Formalizando, queremos: P(X&lt;=5)

``` r
va_poison <- ppois(1:5, lambda = 6)
plot(va_poison)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-22-1.png)<!-- -->

## DITRIBUI√á√ÉO NORMAL

Exemplo: Suponha que a distribui√ß√£o dos sal√°rios dos funcion√°rios de uma
empresa sigam uma distribui√ß√£o normal com m√©dia ùúá = 2.500 e desvio
padr√£o œÉ = 170. Ao selecionar aleatoriamente um indiv√≠duo dessa
popula√ß√£o, qual a probabilidade de ter sal√°rio entre 2.400 e 2.600?
Precisamos achar a probabilidade do indiv√≠duo ter um sal√°rio de at√©
2.600 e subtrair pela probabilidade do indiv√≠duo ter o sal√°rio at√©
2.400.

Para P(X&lt;=2400):

``` r
probabilidade_ate_2400 <- pnorm(q = 2400, mean = 2500, sd = 170)
probabilidade_ate_2400
```

    ## [1] 0.2781872

e para P(X&lt;=2600)

``` r
probabilidade_ate_2600 <- pnorm(q = 2600, mean = 2500, sd =170 )
probabilidade_ate_2600
```

    ## [1] 0.7218128

determinando o intervalo P(X&lt;=2600) - P(X&lt;=2400), temos:

``` r
probabilidade_ate_2600 - probabilidade_ate_2400
```

    ## [1] 0.4436256

Podemos gerar 100 n√∫meros aleat√≥rios para uma distribui√ß√£o normal com
m√©dia 2500 e desvio padr√£o 170

``` r
va_normal <- rnorm(n = 100, mean = 2500,sd = 170)
hist(va_normal)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-26-1.png)<!-- -->

## DISTRIBUI√á√ÉO NORMAL PADR√ÉO

O comando scale() padroniza uma vari√°vel aleat√≥ria. Ao aplicar o comando
na vari√°vel va\_normal que acabmos de criar, ela ficar√° com m√©dia zero e
desvio padr√£o unit√°rio

``` r
va_normal_padrao <- scale(va_normal)
hist(va_normal_padrao)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-27-1.png)<!-- -->

Exemplo: Suponha que a distribui√ß√£o dos sal√°rios dos funcion√°rios de uma
empresa sigam uma distribui√ß√£o normal com m√©dia ùúá = 2.500 e desvio
padr√£o œÉ = 170. Ao selecionar aleatoriamente um indiv√≠duo dessa
popula√ß√£o, qual a probabilidade de ter sal√°rio acima de 2.600?

Padroniza√ß√£o:

``` r
z <- (2600-2500)/170
pnorm(z, mean = 0, sd = 1)
```

    ## [1] 0.7218128

ou simplesmente

``` r
pnorm(z)
```

    ## [1] 0.7218128

Podemos tamb√©m visualizar onde est√° o nosso valor Z em rela√ß√£o a m√©dia

``` r
plot(density(scale(va_normal))) #Plota curva de densidade
abline(v = 0,col = 'blue') #Gera uma linha sobre m√©dia, que √© zero pois padronizamos a distribui√ß√£o
abline(v = 0.58,col = 'red') #Gera uma linha sobre o valor z obtido
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-30-1.png)<!-- -->

## DISTRIBUI√á√ÉO F

Gerando uma amostra aleat√≥ria de 1000 n√∫mero seguindo uma distribui√ß√£o F

``` r
va_f <- rf( n= 1000, df1 = 5 , df2 = 33 ) 
```

Onde: n √© a quantidade de n√∫meros a ser gerado, df1 √© o primeiro grau de
liberidade, df2 √© o segundo grau de liberdade.

Graficamente, temos:

``` r
hist(va_f)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-32-1.png)<!-- -->

V√° aumentando os graus de liberdade e observe como a distribui√ß√£o se
aproxima da normal

``` r
va_f <- rf( n= 1000, df1 = 30 , df2 = 66 )
hist(va_f)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-33-1.png)<!-- -->
aumentando mais ainda

``` r
va_f <- rf( n= 1000, df1 = 60 , df2 = 120 )
hist(va_f)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-34-1.png)<!-- -->

Informa√ß√£o Extra: Uma distribui√ß√£o F √© a raz√£o entre duas chi-quadrado

## DISTRIBUI√á√ÉO T

Gera uma amostra aleat√≥ria de 1000 n√∫meros seguindo uma distribui√ß√£o T

``` r
va_t <- rt(1000, df = 2)
hist(va_t)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-35-1.png)<!-- -->

Observe que a distribui√ß√£o t, assim como a normal padr√£o, √© centrada no
zero. V√° aumentando o grau de liberdade e observando o comportamento do
histograma.

``` r
va_t <- rt(1000, df = 10)
hist(va_t)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-36-1.png)<!-- -->
aumentando mais um pouco:

``` r
va_t <- rt(1000, df = 50)
hist(va_t)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-37-1.png)<!-- -->

## DISTRIBUI√á√ÉO QUI-QUADRADO

Gera uma amostra aleat√≥ria de 1000 n√∫meros seguindo uma distribui√ß√£o
quiquadrado

``` r
va_QuiQuadrado <- rchisq(1000,df = 3)
hist(va_QuiQuadrado)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-38-1.png)<!-- -->

# Cap√≠tulo 03 - Intervalos de confian√ßa

## Intervalo de confian√ßa para m√©dia amostral pela distribui√ß√£o Normal Padr√£o

Obter o intervalo de confian√ßa para uma vari√°vel cuja m√©dia = 30, desvio
padr√£o = 7,31 e n = 30

Temos que definir o n√≠vel de confian√ßa do nosso intervalo. Podemos obter
o valor do quantil para o n√≠vel de confian√ßa desejado com a fun√ß√£o
qnorm().

O quantil na distribui√ß√£o normal padr√£o para 95% de confian√ßa:

``` r
ic <- 0.95
alfa <- 1-ic
1-(alfa/2)
```

    ## [1] 0.975

``` r
qnorm(0.975)
```

    ## [1] 1.959964

Vamos armazenar os valores em objetos

``` r
media <- 30
desvio_padrao_populacional <- 7.31
n <- 30
quantil_95 <- qnorm(0.975)
```

Aplicando a f√≥rmula:

``` r
Limite_Superior <- 30+quantil_95*(desvio_padrao_populacional/sqrt(n))
Limite_Inferior <- 30-quantil_95*(desvio_padrao_populacional/sqrt(n))
paste("Com 95% de confian√ßa, podemos afirmar que a m√©dia varia entre",Limite_Inferior," e ",Limite_Superior)
```

    ## [1] "Com 95% de confian√ßa, podemos afirmar que a m√©dia varia entre 27.3841981618855  e  32.6158018381145"

## Intervalo de confian√ßa para a m√©dia amostral pela distribui√ß√£o t de Student

A teoria nos diz para utilizar a distribui√ß√£o t de Student quando n√£o
soubermos o desvio padr√£o populacional.

Vamos assumir que o desvio padr√£o que temos √© obtido a partir da amostra
e armazenar os valores em objetos:

``` r
media <- 30
desvio_padrao_amostral <- 7.31
n <- 30
quantil_95_t <- qt(0.975,df = n-1)
```

Aplicando a f√≥rmula:

``` r
Limite_Superior_t <- 30+quantil_95_t*(desvio_padrao_amostral/sqrt(n))
Limite_Inferior_t <- 30-quantil_95_t*(desvio_padrao_amostral/sqrt(n))
paste("Com 95% de confian√ßa, podemos afirmar que a m√©dia varia entre",Limite_Inferior_t," e ",Limite_Superior_t)
```

    ## [1] "Com 95% de confian√ßa, podemos afirmar que a m√©dia varia entre 27.2704011402983  e  32.7295988597017"

Supondo que nossa vari√°vel j√° esteja em um data frame aqui no R, tem um
comando para fornecer o intervalo de confian√ßa de forma bem mais f√°cil.

Vamos gerar com o comando rnorm() uma vari√°vel aleatoria com m√©dia 30,
desvio padr√£o 7,31 e n = 30 e vizualizar seu padr√£o:

``` r
va <- rnorm(n = 30, mean = 30, sd = 7.31)
hist(va)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-44-1.png)<!-- -->

Calculando o intervalo de 95% de confian√ßa com a distribui√ß√£o t de
Student com a fun√ßao t.test():

``` r
IC <-t.test(va, conf.level = 0.95)
IC$conf.int
```

    ## [1] 27.02330 32.50328
    ## attr(,"conf.level")
    ## [1] 0.95

Pronto, j√° temos o intervalo de confian√ßa para m√©dia. Beeem mais f√°cil
assim :)

## Intervalo de confian√ßa para a propor√ß√£o

Utilizando o exemplo da apostila, onde calculamos o intervalo para
propor√ß√£o onde 138 de n = 500 clientes realizaram a devolu√ß√£o do
produto.

Vamos armazenar os valores em objetos:

``` r
devolucoes <- 138
n <- 500
quantil_95 <-qnorm(0.975)
proporcao_devolucoes <- devolucoes/n
```

Aplicando a f√≥rmula, temos:

``` r
Limite_Superior_prop <- proporcao_devolucoes + quantil_95 * sqrt(proporcao_devolucoes*(1-proporcao_devolucoes)/n)
Limite_Inferior_prop <- proporcao_devolucoes - quantil_95 * sqrt(proporcao_devolucoes*(1-proporcao_devolucoes)/n)
paste("Com 95% de confian√ßa, podemos afirmar que a propor√ß√£o varia entre",Limite_Inferior_prop," e ",Limite_Superior_prop)
```

    ## [1] "Com 95% de confian√ßa, podemos afirmar que a propor√ß√£o varia entre 0.236817971788424  e  0.315182028211576"

Podemos obter o intervalo de confian√ßa para propor√ß√£o mais f√°cil pela
fun√ß√£o prop.test()

``` r
IC_proporcao <- prop.test(x = 138, n = 500, conf.level = 0.95)
IC_proporcao$conf.int
```

    ## [1] 0.2376893 0.3178132
    ## attr(,"conf.level")
    ## [1] 0.95

## Intervalo de confian√ßa para m√©dia via Bootstrap

Vamos gerar uma va seguindo uma distribui√ß√£o qui-quadrado

``` r
va <- rchisq(n = 60, df = 3)
hist(va)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-49-1.png)<!-- -->

Inicializa variaveis

``` r
medias <- c() #Essa vari√°vel √© um vetor para armazenar a m√©dia de cada subamostra bootstrap
R <- 1000 #Numero de subamostras extra√≠das para gerar a distribui√ß√£o amostral de m√©dias
```

Fazendo o bootstrap:

``` r
for (i in 1:R) {
 #Realiza uma subamostragem aleat√≥ria com reposi√ß√£o da va
reamostra <- sample(va, size = 50, replace = T)
 #Armazena a m√©dia da subamostra
 medias[i] <- mean(reamostra)
}
```

Distribuicao das m√©dias das subamostras (distribui√ß√£o amostral da m√©dia
da va)

``` r
hist(medias)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-52-1.png)<!-- -->

Observe que mesmo a vari√°vel original n√£o seguindo uma distribui√ß√£o
normal, o Teorema Central do Limite nos garante que a distribui√ß√£o das
m√©dias ser√° normal se n √© suficientemente grande.

A partir das m√©dias geradas, precisamos achar dois valores, o que corta
a cauda inferior e o que corta a cauda superior da distribui√ß√£o.
Lembrando que ela √© sim√©trica

Caso o intervalo desejado seja de 95% de confian√ßa, temos que ordenar
essa distribui√ß√£o do menor valor para o maior e achar o valor que
deixar√° 2,5% dos dados para tr√°s e o valor que deixar√° 97,5% para tr√°s:

``` r
(1-0.95)/2
```

    ## [1] 0.025

``` r
1-(1-0.95)/2
```

    ## [1] 0.975

Visualize o intervalo de confian√ßa via bootstrap

``` r
quantile(medias, probs = c(0.025,0.975))
```

    ##     2.5%    97.5% 
    ## 2.507188 3.953352

Vamos realizar mais um experimento: Geraremos uma va com m√©dia = 30 e
desvio padr√£o amostral =7.31 e n = 30

``` r
va <- rnorm(n = 30, mean = 30, sd = 7.31)
```

Iremos calcular o intervalo de confian√ßa usando o Bootstrap e tamb√©m com
a distribui√ß√£o \# t de Student. Compararemos os resultados.

Inicializa variavel para armazenar as m√©dias de cada subamostra, e
realiza o bootstrap:

``` r
medias <- c()
R <- 10000
for (i in 1:R) {
 #Realiza uma subamostragem aleat√≥ria com reposi√ß√£o da va
 reamostra <- sample(va, size = 20, replace = T)
 #Armazena a m√©dia da subamostra
 medias[i] <- mean(reamostra)
}
#Distribuicao das m√©dias das subamostras (distribui√ß√£o amostral da m√©dia da va)
hist(medias)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-56-1.png)<!-- -->

Limites inferior e superior do intervalo pelo bootstrap

``` r
quantile( medias, probs = c(0.025,0.975))
```

    ##     2.5%    97.5% 
    ## 26.52833 32.98991

Limites inferior e superior do intervalo via t de Student

``` r
IC<-t.test(va, conf.level = 0.95)
IC$conf.int
```

    ## [1] 26.94817 32.51941
    ## attr(,"conf.level")
    ## [1] 0.95

# Cap√≠tulo 04 - Testes de Hip√≥teses

## Avaliando a normalidade de uma vari√°vel aleat√≥ria

Gera a vari√°vel aleat√≥ria que segue distribui√ß√£o normal com n = 70,
m√©dia = 40 e desvio padr√£o = 8

``` r
set.seed(10)
va_normal <- rnorm(n = 70, mean = 25, sd = 8)
```

Gera v.a. que segue uma distribui√ß√£o F (n√£o normal) com n = 15, 2 graus
de liberdade no numerados e 10 graus de liberdade no denominador

``` r
va_nao_normal <- rf(n =15, df1 =2, df2 = 10)
```

Visualize o histograma das vari√°veis geradas. Observe como os dados se
distribuem em torno do valor m√©dio na va normal

``` r
hist(va_normal)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-61-1.png)<!-- -->

Observe como os dados n√£o se distribuem em torno de um valor m√©dio
exibindo padr√£o assim√©trico

``` r
hist(va_nao_normal)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-62-1.png)<!-- -->

Visualize o QQ-Plot. Observe como os pontos de dados seguem a linha reta
qq norm da va normal

``` r
qqnorm(va_normal)
qqline(va_normal) #Este comando √© para adicionar a linha
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-63-1.png)<!-- -->

Observe como os pontos de dados n√£o seguem a linha reta na va n√£o normal

``` r
qqnorm(va_nao_normal)
qqline(va_nao_normal) #Este comando √© para adicionar a linha
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-64-1.png)<!-- -->

Vamos aplicar o teste de hip√≥teses Shapiro Wilk. O teste funciona sob as
hip√≥teses H0: A vari√°vel segue uma distribui√ß√£o normal H1: A vari√°vel
n√£o segue uma distribui√ß√£o normal

Fixe um n√≠vel de signific√¢ncia alfa e analise o p valor (p-value) do
Shapiro Wilk. Se o p-value for menor que alfa a hip√≥tese nula deve ser
rejeitada

``` r
shapiro.test(va_normal)
```

    ## 
    ##  Shapiro-Wilk normality test
    ## 
    ## data:  va_normal
    ## W = 0.99041, p-value = 0.8742

``` r
shapiro.test(va_nao_normal)
```

    ## 
    ##  Shapiro-Wilk normality test
    ## 
    ## data:  va_nao_normal
    ## W = 0.78348, p-value = 0.002274

## Teste t para diferen√ßa de m√©dias (duas amostras independentes)

Iremos simular o exemplo da apostila. Iremos testar se: H0: As vendas na
posi√ß√£o A s√£o iguais as Vendas na Posi√ß√£o B H1: As vendas na posi√ß√£o A
s√£o diferentes das vendas na posi√ß√£o B

``` r
mu1 <- 150.1 #Armazena as m√©dia de vendas na posi√ß√£o A
mu2 <- 182.1 #Armazena as m√©dia de vendas na posi√ß√£o B

s1 <- 17 #Armazena o desvio padr√£o das vendas na posi√ß√£o A
s2 <- 19.2 #Armazena o desvio padr√£o das vendas na posi√ß√£o B

n1 <- 25 #Armazena a quantidade observa√ß√µes registradas para de vendas na posi√ß√£o A
n2 <- 30 #Armazena a quantidade observa√ß√µes registradas para de vendas na posi√ß√£o B
```

Calcula nossa estat√≠stica de teste. Que √© o t calculado:

``` r
t <- (mu1 - mu2) / sqrt( s1^2/n1 + s2^2/n2)
t #Visualize o valor de t calculado
```

    ## [1] -6.552756

Calcula os graus de liberdade da estat√≠stica de teste

``` r
gl <- (s1^2/n1 + s2^2/n2)^2 /( (s1^2/n1)^2 / (n1-1) + (s2^2/n2)^2 / (n2-1) )
gl #Visualize a quantidade de graus de liberdade
```

    ## [1] 52.78313

Obtem o quantil (t cr√≠tico) para uma distribui√ß√£o t com gl graus de
liberdade. A um alfa de 5%

``` r
quantil <- qt(0.975,df = gl)
quantil #Visualize o t cr√≠tico
```

    ## [1] 2.005938

Esse √© o aspecto de uma distribui√ß√£o t com n=53 observa√ß√µes e com n - 1
graus de liberdade

Observe onde est√£o os valores cr√≠ticos que acabamos de encontrar

``` r
plot(density(rt(n = 53,df = gl)),xlim = c(-7,7)) 
abline(v = quantil,col = 'blue',lwd = 2)
abline(v = -quantil,col = 'blue',lwd = 2)
abline(v = t, col = 'red')# Observe como o tcalculado √© muito menor que o tcr√≠tico. Est√° na regi√£o de rejei√ß√£o
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-70-1.png)<!-- -->

Obtendo o valor p

P(Tcalculado &gt; Tcritico)

``` r
2*pt(q = t, df = gl)
```

    ## [1] 2.403372e-08

Agora vamos realizar o mesmo teste de hip√≥tese utilizando a fun√ß√£o
nativa do R, **t.test()**

Observe no output desta fun√ß√£o, que ela j√° nos da tudo pronto, t
calculado e valor p

``` r
vendas_A <- rnorm(n= 25, mean = 150.1, sd = 17)
vendas_B <- rnorm(n = 30, mean = 182.1, sd = 19.2)
t.test(vendas_A,vendas_B, alternative = 'two.sided')
```

    ## 
    ##  Welch Two Sample t-test
    ## 
    ## data:  vendas_A and vendas_B
    ## t = -5.8545, df = 49.638, p-value = 3.764e-07
    ## alternative hypothesis: true difference in means is not equal to 0
    ## 95 percent confidence interval:
    ##  -48.84840 -23.88916
    ## sample estimates:
    ## mean of x mean of y 
    ##  150.5147  186.8835

Esse √© o aspecto de uma distribui√ß√£o t com n observa√ß√µes e com n - 1
graus de liberdade

``` r
n <- 5
plot(density(rt(n = n,df = n-1))) 
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-73-1.png)<!-- -->

Altere o valor de n de 5 em 5 observe que a medida que os graus de
liberdade aumenta a distribui√ß√£o se aproxima da normal. Como os valores
s√£o geradosa leatoriamente poderemos ter curvas diferentes para um mesmo
valor de n, mas a medida que n cresce o comportamento sim√©trico tende a
estabilizar

``` r
n <- 10
plot(density(rt(n = n,df = n-1))) 
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-74-1.png)<!-- -->

``` r
n <- 15
plot(density(rt(n = n,df = n-1))) 
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-75-1.png)<!-- -->

## Teste t para diferen√ßa de m√©dias (duas amostras dependentes)

Iremos simular o exemplo da apostila: H0: O peso m√©dio ap√≥s a dieta √©
igual ao peso m√©dio antes da dieta H1: O peso m√©dio ap√≥s a dieta √© menor
do que o peso m√©dio antes da dieta

Iremos utilizar uma biblioteca adicional para gerar valores aleat√≥rios
que sigam uma distribui√ß√£o normal entre um intervalo de valor para
simular os pesos

A biblioteca chama ‚Äòtruncnorm‚Äô. Basta instalar com o comando abaixo
**install.packages()**.

Uma vez instalada n√£o h√° mais necessidade de instalar novamente. Basta
carregar com o comando **library()**

``` r
#install.packages('truncnorm')
library(truncnorm)
set.seed(100)
```

Gera uma amostra aleat√≥ria, seguindo uma distribui√ß√£o normal cujo valor
m√≠nimo √© 100 e o valor m√°ximo √© 140.

O valor de n=20, m√©dia = 123 e desvio padr√£o 18

Com essa v.a. iremos simular os pesos dos indiv√≠duos antes da dieta

``` r
antes_da_dieta <- rtruncnorm(n=20, a=100, b=140, mean=123, sd=18)
```

Gera uma amostra aleat√≥ria, seguindo uma distribui√ß√£o normal cujo valor
m√≠nimo √© 110 e o valor m√°ximo √© 130.

O valor de n=20, m√©dia = 110 e desvio padr√£o 28

Com essa v.a. iremos simular os pesos dos indiv√≠duos ap√≥s a dieta

``` r
depois_da_dieta <- rtruncnorm(n=20, a=110, b=130, mean=110, sd=28)
```

Calcula e visualiza a diferen√ßa depois da dieta e antes da dieta, para
cada indiv√≠duo

``` r
diferenca <- depois_da_dieta-antes_da_dieta
hist(diferenca)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-79-1.png)<!-- -->

Avalie a normalidade da distribui√ß√£o da diferen√ßa

``` r
shapiro.test(diferenca)
```

    ## 
    ##  Shapiro-Wilk normality test
    ## 
    ## data:  diferenca
    ## W = 0.9697, p-value = 0.7485

Aplica test t com os seguintes argumentos

``` r
t.test(depois_da_dieta,antes_da_dieta,
 paired = TRUE, #Pareado
 alternative = "less", #Unilateral a esquerda
 conf.level = 0.9 #90 porcento de confian√ßa
 ) 
```

    ## 
    ##  Paired t-test
    ## 
    ## data:  depois_da_dieta and antes_da_dieta
    ## t = -0.95449, df = 19, p-value = 0.1759
    ## alternative hypothesis: true difference in means is less than 0
    ## 90 percent confidence interval:
    ##       -Inf 0.8496348
    ## sample estimates:
    ## mean of the differences 
    ##               -2.172799

O comando **t.test()** acima nos da tudo que precisamos para executar e
concluir o teste. Mas a t√≠tulo de conhecimento, podemos realizar o teste
passo a passo:

``` r
#Calcula a m√©dia das diferen√ßas
media <- mean(diferenca)

#Desvio padr√£o das diferen√ßas
desvio_padrao <- sd(diferenca)

#Quantidade de indiv√≠duos
n <- 20

#Obtem o t calculado
t_calculado <- media / (desvio_padrao/sqrt(n))

#Obtem o valor p para o t calculado com n - 1 graus de liberdade.
pt(q = t_calculado, df = n-1) 
```

    ## [1] 0.1759111

Podemos tamb√©m obter o t cr√≠tico para uma distribui√ß√£o t com 19 (n-1 =
20-1) graus de liberdade ao n√≠vel de confian√ßa de 90%

``` r
tcr√≠tico_teste_t_pareado <- -qt(p = 0.9, df = 19) #Devido ao teste ser unilateral a esquerda a distribui√ß√£o t ser sim√©trica, nossa estat√≠stica de teste ser√° negativa
```

Observe que o t calculado √© maior que o t critico. Como estamos em um
teste unilateral a esquerda o t calculado estar√° fora da regi√£o de
rejei√ß√£o caso seja maior que o t cr√≠tico

``` r
t_calculado < tcr√≠tico_teste_t_pareado
```

    ## [1] FALSE

## Teste Qui-Quadrado para associa√ß√£o entre vari√°veis categ√≥ricas

Iremos simular o exemplo da apostila H0: O fato do cliente estar ou n√£o
com crian√ßa n√£o tem rela√ß√£o com o fato de comprar ou n√£o comprar H1: O
fato do cliente estar ou n√£o com crian√ßa tem rela√ß√£o com fato de comprar
ou n√£o comprar

Vamos gerar um data frame contendo os dados da pesquisa:

``` r
dados <- data.frame(Cliente = c("Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto", "Adulto", "Adulto", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto", "Adulto", "Adulto", "Adulto",
 "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto_com_Crianca", "Adulto", "Adulto_com_Crianca", "Adulto",
 "Adulto", "Adulto_com_Crianca", "Adulto_com_Crianca", "Adulto_com_Crianca",
 "Adulto", "Adulto_com_Crianca", "Adulto", "Adulto", "Adulto",
 "Adulto","Adulto","Adulto","Adulto","Adulto","Adulto"),

  Comprou = c("N√£o_Comprou", "N√£o_Comprou", "N√£o_Comprou", "N√£o_Comprou",
 "N√£o_Comprou", "N√£o_Comprou", "Comprou", "Comprou", "Comprou",
 "Comprou", "Comprou", "Comprou", "Comprou", "Comprou", "Comprou",
 "Comprou", "Comprou", "Comprou", "Comprou", "Comprou", "Comprou","Comprou", "Comprou", "Comprou", "N√£o_Comprou", "N√£o_Comprou",
 "N√£o_Comprou", "N√£o_Comprou", "Comprou", "N√£o_Comprou", "Comprou",
 "Comprou", "N√£o_Comprou", "N√£o_Comprou", "N√£o_Comprou",
"N√£o_Comprou",
 "N√£o_Comprou", "Comprou", "Comprou", "N√£o_Comprou", "N√£o_Comprou",
 "N√£o_Comprou", "N√£o_Comprou", "N√£o_Comprou",
"Comprou","Comprou","Comprou","Comprou","Comprou","Comprou")
)
```

Visualizando o conjunto de dados:

``` r
View(dados)
```

Gera tabela de contig√™ncia 2x2

``` r
tabela <- table(dados$Cliente,dados$Comprou)
tabela
```

    ##                     
    ##                      Comprou N√£o_Comprou
    ##   Adulto                   6          14
    ##   Adulto_com_Crianca      23           7

Visualiza a tabela num gr√°fico de barras

``` r
barplot(tabela)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-88-1.png)<!-- -->

O valor cr√≠tico para uma distribui√ß√£o qui-quadrado com (linhas-1) \*
(colunas-1) = 1 grau de liberdade ao n√≠vel de confian√ßa de 95%

``` r
qchisq(p=0.95,df = 1)
```

    ## [1] 3.841459

O valor p unilateral fica

``` r
1-pchisq(q=10.728,df=1)
```

    ## [1] 0.001055264

Mesmo que o n√≠vel de confian√ßa fosse 99%, ainda ter√≠amos evid√™ncias para
rejeitar H0

Assim como fizemos no test t, podemos usar um comando direto no R para
realizar o teste qui-quadrado **chisq.test()**

``` r
teste<-chisq.test(tabela,correct = F)
teste
```

    ## 
    ##  Pearson's Chi-squared test
    ## 
    ## data:  tabela
    ## X-squared = 10.728, df = 1, p-value = 0.001055

Visualiza valores observados. Que nada mais √© do que a tabela original

``` r
teste$observed
```

    ##                     
    ##                      Comprou N√£o_Comprou
    ##   Adulto                   6          14
    ##   Adulto_com_Crianca      23           7

Visualiza valores esperados

``` r
teste$expected
```

    ##                     
    ##                      Comprou N√£o_Comprou
    ##   Adulto                11.6         8.4
    ##   Adulto_com_Crianca    17.4        12.6

## ANOVA (An√°lise de Vari√¢ncia)

Vamos utilizar o exemplo da apostila: H0: N√£o h√° diferen√ßa no valor
m√©dio gasto com bebidas em nenhuma das popula√ß√µes H1: H√° diferen√ßa no
valor m√©dio gasto com bebidas em pelo menos uma das popula√ß√µes

Geramos um data frame contendo os dados da pesquisa:

``` r
dados_anova <- data.frame(Gastos = c(174.770021661909, 161.329206619394,
 153.679900850863, 163.790338797433, 141.363480335882,
175.351592994046, 185.793398289321, 184.720273514352, 163.400459287948,
170.202462740626, 150.8549565713, 167.583106239899, 140.190492201897,
157.440088617225, 171.596654773339, 138.885665257324, 147.942698809323,
9.87474262516482, 50.5645554670016, 14.2586307887884, 8.5061846804934,
25.0875496696788, 17.0661987504312, 41.3867417301938, 20.8113941426179,
60.1224674502026, 35.5154028285664, 23.7622285692359, 34.6086119259266,
30.4321086925016, 27.8188980544904, 37.4729772794009, 30.7229538650678,
48.0452539322412, 78.9197865324734, 42.4926762466659, 8.81227865272712,
39.5751781629677, 37.1329656327517, 15.8016718071775, 5.74735216885902,
38.684069121093, 30.9398891106907, 34.7370783113952, 13.2630510987537,
19.6212096123791, 16.716945267481, 24.4037922212213, 4.63398786180773,
32.9436217626275, 21.511905851158, 31.4997283634204, 26.6610570873775,
34.6304034101472, 16.2704826042681, 11.2323425300881, 18.023244405391,
15.4790632095655, 8.25633422881043, 27.9053307974433, 72.3298402892867,
4.7263338963663, 14.4153129255327, 41.2234268777169, 50.5684226296565,
19.8344282661234, 8.81306901471397, 19.5112436004646, 55.6251926080436,
16.7592556127806, 20.3176176298076, 31.2073058210955, 17.0613250010048,
47.8590627884627, 2.59778754862417, 35.9470130480825, 2.39404093355522,
9.38425601777391, 25.2455048267186, 16.1960287769175, 43.530118783298,
32.7250288712979, 5.43268078364765, 44.5365791890593, 32.9831443965413,
28.2104605365607, 3.18609515001209, 14.3698142789208, 39.9617218607622,
50.564581262513, 10.4634451365926, 36.4842442182048, 13.1330189654278,
8.93702642184252, 12.1501174131844, 22.2552757873296, 15.1407470062459,
11.7525513477354, 16.2990775324815, 24.4627568806115, 2.87916580644454,
44.5453919973285, 38.0393535792355, 32.1985589022666, 0.357075783631849,
22.0703974352325, 50.7486034030794, 18.604230207709, 5.83122133978906,
19.9252025339318, 6.8366108202567, 27.5834177510951, 41.9303025963975,
3.077799353254, 28.0507001837521, 33.0042729903, 50.7366690908169,
30.1697285113061, 6.53184416916073, 7.53469171526227, 5.49225229796712,
9.53198727121377, 6.59266645551752, 19.8423174628847, 0.781567028951091,
22.1605754480815, 5.90830712162365, 54.3457453874529, 33.3341495203441,
37.2034845899045
), Estado_Civil = c("solteiros", "solteiros", "solteiros", "solteiros",
 "solteiros", "solteiros", "solteiros", "solteiros", "solteiros",
 "solteiros", "solteiros", "solteiros", "solteiros", "solteiros",
 "solteiros", "solteiros", "solteiros", "Casados", "Casados", "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Casados", "Casados", "Casados", "Casados", "Casados", "Casados",
 "Divorciados", "Divorciados", "Divorciados", "Divorciados", "Divorciados",
 "Divorciados", "Divorciados", "Divorciados", "Divorciados", "Divorciados",
 "Divorciados", "Divorciados", "Divorciados", "Divorciados",
"Divorciados"))
```

Visualiza o conjunto de dados

``` r
View(dados_anova)
```

Podemos utilizar os recursos de visualiza√ß√£o da biblioteca **ggplot2**
para visualizar a distribui√ß√£o dos gastos nas popula√ß√µes

``` r
#install.packages("ggplot2")
library(ggplot2)
ggplot(data = dados_anova, aes(x = Gastos, fill = Estado_Civil)) +
 geom_density(alpha=0.4)+
 xlim(-50,300)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-96-1.png)<!-- -->

√â bastante comum tamb√©m analisarmos a variabilidade nas distintas
popula√ß√µes com uso de boxplot

``` r
boxplot(dados_anova$Gastos ~ dados_anova$Estado_Civil, ylab="Gastos", xlab="Estado Civil")
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-97-1.png)<!-- -->

Com o comando **aov()**, o R gera a tabela da ANOVA completa

``` r
anova <- aov(Gastos~ #Vari√°vel resposta
 Estado_Civil, #Fator que queremos testar se exerce influencia na vari√°vel resposta
 data = dados_anova)
```

Visualize a tabela da ANOVA. Observe o F calculado e o valor p ( Pr &gt;
F)

``` r
summary(anova)
```

    ##               Df Sum Sq Mean Sq F value Pr(>F)    
    ## Estado_Civil   2 276639  138319   537.8 <2e-16 ***
    ## Residuals    127  32665     257                   
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

O valor p √© praticamente zero. Mesmo que nosso n√≠vel de confian√ßa fosse
99,9% ainda ter√≠amos evid√™ncias para rejeitar H0

## Cap√≠tulo 05 - Regress√£o Linear

Instala e carrega biblioteca para gerar a curva ROC

``` r
#install.packages('pROC') #Instala
library("pROC") #Carrega
```

    ## Type 'citation("pROC")' for a citation.

    ## 
    ## Attaching package: 'pROC'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     cov, smooth, var

Monte o dataset

``` r
dados <- data.frame(Prova_Logica = c(2, 2, 5, 5, 5, 2, 3, 2, 1, 4,  5, 8, 1, 1, 3, 4, 3, 2, 1, 1, 8, 8, 1, 2, 1, 5, 3, 3, 5, 4, 4, 1, 8, 3, 2, 3, 3, 2, 1, 1, 5, 4, 1, 5, 3, 1, 4, 6, 1, 1, 8, 1, 1, 5, 1, 5, 3, 1, 1, 8, 1, 1, 1, 1, 1, 2, 1, 5, 5, 4, 2, 1, 8, 4, 5, 1, 3, 3, 3, 5, 3, 1, 7, 1, 1, 2, 9, 5, 3, 1, 5, 1, 4, 2, 1, 4, 3, 3, 8, 1, 1, 8, 5, 1, 1, 1, 5, 8, 5, 1, 4, 2, 5, 4, 5, 3, 3, 5, 5, 5, 5, 5, 8, 5, 4, 9, 8, 1, 3, 4, 2, 5, 1, 4, 3, 5, 5, 5, 6, 4, 3, 5, 7, 1, 8, 5, 7, 3, 2, 3, 2, 5, 5, 5, 5, 4, 4, 8, 1, 1, 2, 5, 3, 2, 7, 4, 1, 1, 1, 4, 5, 1, 1, 8, 3, 6, 8, 3,1, 3, 3, 2, 8, 4, 1, 1, 1, 1, 1, 2, 3, 4, 6, 2, 3, 3, 4, 2, 1, 5, 2, 4, 3, 3, 1, 3, 3, 3, 1, 3, 5, 6, 1, 5, 1, 5, 4, 3, 1, 6, 1, 4, 9, 3, 3, 2, 1, 1, 4, 3, 1, 3, 1, 1, 3, 7, 8, 1, 3, 5, 6, 3, 6, 5, 8, 5, 1, 1, 4, 2, 1, 8, 7, 5, 1, 1, 1, 6, 5, 7, 3, 3, 5, 1, 3, 5, 1, 8, 8, 1, 2, 3, 3, 3, 3, 7, 1, 9, 8, 4, 1, 7, 1, 1, 1, 5, 1, 1, 5, 3, 5, 1, 3, 6, 2, 1, 3, 4, 5, 6, 1, 5, 1, 5, 1, 1, 5, 1, 1, 1, 5, 1, 3, 7, 1, 4, 3, 7, 1, 1, 5, 4, 1, 1, 3, 5, 4, 2, 1, 5, 1, 1, 1, 8, 8, 5, 1, 2, 1, 6, 8, 3, 1, 5, 1, 5, 1, 4, 4, 8, 1, 1, 1, 5, 1, 1, 5, 4, 6, 8, 1, 3, 1, 6, 1, 1, 1, 1, 1, 8, 1, 5, 3, 1, 4, 4, 7, 2, 3, 3, 5, 8, 3, 1, 4, 1, 5, 1, 7, 2, 6, 4, 1, 3, 1, 8, 5, 5, 5, 3, 1, 4, 5, 3, 6, 1, 3, 3, 5, 4, 5, 3, 1, 4, 5, 1, 3, 6, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 5, 4, 8, 1, 5, 6, 6, 4, 2, 5, 5, 6, 1, 2, 5, 6, 4, 2, 1, 2, 7, 2, 8, 1, 3, 1, 1, 1, 6, 1, 4, 3, 1, 5, 2, 1, 1, 5, 4, 3, 1, 3, 1, 1, 2, 4, 5, 4, 5, 4, 3, 7, 4, 1, 7, 1, 8, 5, 3, 3, 5, 1, 2, 1, 5, 4, 4, 4, 3, 6, 8, 8, 1, 1, 1, 8, 8, 1, 5, 1, 4, 5, 1, 4, 1, 4, 1, 5, 1, 4, 4, 1, 1, 2, 5, 1, 4, 4, 5, 4, 1, 1, 5, 3, 5, 4, 1, 1, 5, 3, 3, 1, 5, 5, 4, 5, 7, 2, 5, 9, 4, 6, 1, 1, 1, 1, 8, 7, 2, 3, 2, 9, 3, 1, 5, 1, 3, 5, 1, 4, 6, 3, 1, 5, 1, 3, 9, 1, 4,
 8, 8, 1, 3, 2, 3, 3, 1, 5, 1, 1, 3, 1, 5, 3, 4, 3, 4, 1, 4, 3, 5, 1, 5, 2, 5, 8, 9, 4, 7, 4, 8, 5, 7, 5, 3, 5, 6, 3, 1, 5, 6, 4, 3, 5, 1, 2, 4, 1, 1, 2, 9, 5, 1, 5, 1, 2, 1, 5, 1, 2, 3, 5, 1, 1, 5, 3, 9, 5, 6, 9, 6, 1, 1, 9, 1, 3, 5, 4, 8, 4, 2, 7, 1, 6, 3, 7, 8, 1, 5, 5, 6, 1, 4, 4, 3, 1, 5, 5, 8, 3, 3, 1, 9, 5, 5, 5, 5, 1, 9, 6, 3, 1, 1, 2, 4, 6, 5, 7, 6, 5, 1), 
 
 Redacao = c(1, 1, 1, 4, 3, 3, 5, 5, 9, 1, 1, 1, 1, 1, 4, 3, 3, 1, 1, 1, 1, 7, 1, 1, 8, 1, 1, 1, 3, 8, 3, 1, 5, 3, 3, 1, 2, 7, 1, 1, 1, 1, 1, 1, 7, 1, 1, 8, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 8, 5, 1, 1, 1, 1, 1, 2, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 4, 1, 8, 5, 1, 5, 8, 1, 1, 1, 5, 1, 1, 1, 2, 3, 3, 1, 3, 8, 1, 4, 6, 1, 1, 1, 1, 3, 1, 1, 2, 3, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 8, 1, 5, 1, 1, 1, 1, 1, 3, 6, 1, 2, 5, 6, 1, 2, 2, 1, 8, 1, 4, 6, 9, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 3, 5, 1, 1, 1, 1, 3, 1, 4, 1, 1, 1, 2, 1, 3, 1, 1, 1, 4, 5, 1, 1, 6, 1, 3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 6, 1, 8, 1, 1, 5, 1, 8, 2, 6, 1, 5, 1, 6, 1, 1, 1, 1, 1, 1, 1,1, 3, 1, 4, 8, 1, 1, 1, 8, 1, 3, 1, 6, 3, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 5, 1, 1, 3, 1, 1, 7, 4, 1, 1, 1, 1, 6, 1, 3,1, 4, 1, 1, 7, 2, 6, 4, 1, 1, 1, 1, 1, 4, 7, 1, 3, 1, 1, 9, 1,1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 3, 1, 1, 3, 2, 1, 1, 1, 5, 3, 1,1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 8, 1, 7, 1,1, 3, 8, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 2, 7, 1, 3, 1, 1, 1, 4,2, 4, 2, 2, 5, 3, 1, 1, 1, 5, 1, 9, 1, 1, 3, 2, 1, 1, 5, 1, 2,1, 3, 8, 1, 5, 1, 4, 3, 1, 8, 1, 6, 5, 1, 1, 1, 1, 1, 4, 5, 1,7, 8, 1, 4, 1, 1, 1, 1, 4, 1, 1, 2, 1, 8, 2, 6, 2, 1, 4, 1, 1,1, 1, 1, 4, 1, 1, 1, 1, 1, 8, 1, 1, 1, 3, 1, 1, 1, 8, 1, 1, 1,3, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1,3, 1, 2, 7, 2, 1, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 3, 1, 1, 5, 1,7, 1, 1, 1, 3, 6, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 6, 8, 8, 7, 2,1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 5, 1, 1, 1, 1, 9, 1, 8, 1, 1, 2,4, 1, 1, 6, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 6, 1, 2,1, 1, 5, 4, 1, 8, 4, 6, 6, 1, 1, 1, 9, 1, 1, 1, 1, 1, 8, 1, 1,1, 1, 1, 3, 1, 1, 4, 1, 1, 3, 4, 1, 1, 3, 2, 3, 1, 2, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 4, 1, 4, 2, 1, 6, 1,
4, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, 6, 1, 1, 1, 3, 2, 8, 1, 1,1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 6, 1, 6, 7, 1, 1, 5, 1, 2, 5,1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 8, 7, 1, 1, 1, 1, 4, 1, 6, 1,2, 8, 4, 7, 1, 1, 1, 5, 1, 1, 2, 1, 1, 7, 1, 1, 1, 4, 1, 1, 3,1, 5, 1, 7, 1), 

Auto_Avaliacao = c(1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 9, 1, 1, 1, 3, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 9, 1, 1, 1, 1, 1, 1, 8, 1, 1, 9, 7, 1, 2,
 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 3, 8, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 8, 3, 1, 6, 1, 6, 1, 1, 3, 1, 1,1, 1, 8, 5, 3, 3, 1, 1, 3, 1, 1, 1, 1, 1, 6, 1, 2, 1, 1, 1, 1, 1, 4, 1, 6, 1, 1, 1, 2, 3, 1, 4, 7, 6, 1, 1, 1, 1, 1, 1, 9, 1, 2, 1, 1, 1, 1, 2, 1, 8, 1, 8, 1, 3, 2, 1, 1, 1, 1, 2, 6, 1, 1, 1, 2, 1, 3, 1, 1, 8, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 1, 5, 1, 1, 1, 1, 1, 5, 1, 1, 1, 3, 5, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 6, 1, 8, 1, 2, 5, 2, 1, 1, 7, 1, 1, 1, 8, 1, 1, 5, 1, 1, 1, 3, 1, 1, 1, 9, 1, 1, 1, 5, 9, 1, 5, 3, 4, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, 3, 8, 1, 4, 1, 4, 1, 1, 1, 6, 1, 3, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 9, 1, 1, 2, 2, 8, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 4, 3, 1, 1, 4, 1, 6, 2, 1, 5, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 8, 3, 4, 8, 1, 3, 7, 7, 1, 1, 2, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 9, 1, 1, 1, 1, 8, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 9, 6, 3, 3, 1, 2, 1, 8, 7, 1, 1, 1, 1, 1, 6, 3, 1, 4, 5, 1, 6, 1, 1, 1, 1, 3, 1, 1, 2, 1, 6, 3, 7, 1, 8, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 9, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 7, 1, 1, 4, 2, 1, 5, 1, 5, 1, 1, 1, 4, 6, 1, 1, 4, 1, 2, 1, 1, 1, 9, 8, 1, 9, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 3, 1, 1, 8, 1, 1, 6, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 5, 1, 6, 3, 1, 4, 3, 1, 7, 5, 1, 1, 1, 1, 1, 1, 2, 1, 5, 9, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 8, 1, 1, 2, 5, 1, 1, 5, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 5, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 6, 1, 9, 5, 3, 1, 8, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 8, 1, 1, 1, 6, 1, 1, 1, 1, 9, 1, 1, 1),

Classe = c("Ruim",  "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim",  "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Ruim",  "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim",  "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa",  "Boa", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim",  "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim",  "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim",  "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim",  "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim",  "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim",  "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim",  "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim",  "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim",  "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa",  "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim",
 "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim",  "Boa", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa",  "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim","Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Boa", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim",
 "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim",
 "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Boa", "Boa", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Boa", "Boa", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Ruim", "Ruim", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Boa", "Ruim", "Ruim", "Boa", "Ruim", "Boa", "Boa", "Boa", "Ruim"))
```

Converte variavel resposta para factor

``` r
dados$Classe <- factor(dados$Classe, levels = c('Ruim','Boa'))
```

Pequena analisa exploratoria, com a biblioteca **dplyr**

``` r
#install.packages("dplyr")
library("dplyr")
```

    ## 
    ## Attaching package: 'dplyr'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     filter, lag

    ## The following objects are masked from 'package:base':
    ## 
    ##     intersect, setdiff, setequal, union

``` r
dados %>% group_by(Classe) %>% summarise_all("mean")
```

    ## # A tibble: 2 x 4
    ##   Classe Prova_Logica Redacao Auto_Avaliacao
    ##   <fct>         <dbl>   <dbl>          <dbl>
    ## 1 Ruim           2.96    1.33           1.29
    ## 2 Boa            4.62    4.07           3.59

Ajusta a regress√£o log√≠stica

``` r
fit <- glm(Classe ~ Prova_Logica + Redacao + Auto_Avaliacao ,
 data = dados,
 family = binomial)
```

Visualiza resumo do modelo ajustado

``` r
summary(fit)
```

    ## 
    ## Call:
    ## glm(formula = Classe ~ Prova_Logica + Redacao + Auto_Avaliacao, 
    ##     family = binomial, data = dados)
    ## 
    ## Deviance Residuals: 
    ##     Min       1Q   Median       3Q      Max  
    ## -3.4470  -0.5346  -0.3941   0.2987   2.2777  
    ## 
    ## Coefficients:
    ##                Estimate Std. Error z value Pr(>|z|)    
    ## (Intercept)    -3.84584    0.27818 -13.825  < 2e-16 ***
    ## Prova_Logica    0.21431    0.05186   4.132 3.59e-05 ***
    ## Redacao         0.69144    0.07437   9.298  < 2e-16 ***
    ## Auto_Avaliacao  0.42384    0.06730   6.298 3.02e-10 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 900.53  on 698  degrees of freedom
    ## Residual deviance: 526.77  on 695  degrees of freedom
    ## AIC: 534.77
    ## 
    ## Number of Fisher Scoring iterations: 5

Aplica exponenciacao nos coeficientes para interpretar

``` r
exp(fit$coefficients)
```

    ##    (Intercept)   Prova_Logica        Redacao Auto_Avaliacao 
    ##     0.02136846     1.23900922     1.99658084     1.52781739

Curva ROC

``` r
prob = predict(fit, newdata = dados, type = "response")
roc = roc(dados$Classe ~ prob, plot = TRUE, print.auc = TRUE)
```

    ## Setting levels: control = Ruim, case = Boa

    ## Setting direction: controls < cases

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-107-1.png)<!-- -->

Obtem a predicao/probabilidade para cada observacao

``` r
Probabilidade <- predict(fit, newdata= dados,type = 'response')
```

Se a probabilidade for maior que 50% classifica como ‚ÄòBoa‚Äô

``` r
Classe_Predita <- ifelse(Probabilidade > 0.5,"Boa","Ruim")
```

Visualiza data frame com as predicoes

``` r
View(data.frame(dados,Probabilidade,Classe_Predita))
```

Gera matriz de confusao

``` r
confusao <- table(Classe_Predita = Classe_Predita, Classe_Original = relevel(dados$Classe,ref = 'Boa'))
confusao
```

    ##               Classe_Original
    ## Classe_Predita Boa Ruim
    ##           Boa  169   23
    ##           Ruim  72  435

Armazena valores da matriz de confusao

``` r
vp <- confusao[1,1];vp
```

    ## [1] 169

``` r
fn <- confusao[2,1];fn
```

    ## [1] 72

``` r
vn <- confusao[2,2];vn
```

    ## [1] 435

``` r
fp <- confusao[1,2];fp
```

    ## [1] 23

Calcula acuracia

``` r
acuracia <- sum(diag(confusao))/ sum(confusao);acuracia
```

    ## [1] 0.8640916

Calcula Sensitividade

``` r
sensitividade <- vp /(vp+fn)
sensitividade
```

    ## [1] 0.7012448

Cacula Especificidade

``` r
especificidade <- vn / (vn + fp) 
especificidade
```

    ## [1] 0.9497817

Analise de Sensitividade e Especificidade

(Todo o trecho de c√≥digo abaixo n√£o est√° funcionando perfeitamente e eu
n√£o sei exatamente o porque. Est√° exatamente igual ao trecho usado pelo
professor na video aula. Se algu√©m tiver uma luz, eu agrade√ßo!)

``` r
limiares <- sort(Probabilidade)
acuracia <- c()
sensitividade <- c()
especificidade <- c()

for ( i in 1:length(limiares)) {
  limiar_atual <- limiares[i]
  Classe_Predita <- ifelse(Probabilidade > limiar_atual, 'Boa', 'Ruim')

#Gera matriz de confusao
  confusao <- table(Classe_Predita = Classe_Predita, Classe_Original = relevel(dados$Classe, ref = 'Boa'))
}
confusao
```

    ##               Classe_Original
    ## Classe_Predita Boa Ruim
    ##           Ruim 241  458

Armazena valores da matriz de confusao (sinceramente n√£o sei qual √© o
erro nesse trecho. por causa dele, os pr√≥ximos trechos ficar√£o
diferentes do que o professor utilizou em aula)

``` r
vp <- confusao[1,1];vp
```

    ## [1] 241

``` r
#fn <- confusao[2,1];fn
#vn <- confusao[2,2];vn
fp <- confusao[1,2];fp
```

    ## [1] 458

Calcula acuracia

``` r
acuracia[i] <- sum(diag(confusao))/ sum(confusao);
```

Calcula Sensitividade

``` r
sensitividade[i] <- vp /(vp+fn)
```

Calcula Especificidade

``` r
especificidade[i] <- vn / (vn + fp) 
```

Teoricamente, deveria mostrar o gr√°fico, mas n√£o funciona

    plot(y = sensitividade[1:698] , x = limiares[1:698], type="l", col="red", ylab =
    'Sensitividade e Especificidade', xlab= 'Pontos de Corte')
    grid()

    lines(y = especificidade[1:698], x = limiares[1:698], type = 'l',col="blue" )
    legend("bottomleft", c("sensibilidade","especificidade"),
     col=c("red","blue"), lty=c(1,1),bty="n", cex=1, lwd=1)
    abline(v=0.225)

Obtem novamente as probabilidades para classificar baseado no ponto de
corte 22,5%

``` r
Probabilidade <- predict(fit, newdata= dados,type = 'response')
Classe_Predita <- ifelse(Probabilidade > 0.225,"Boa","Ruim")
View(data.frame(dados,Probabilidade,Classe_Predita))
```

Visualiza matriz de confusao final

``` r
confusao <- table(Classe_Predita = Classe_Predita, Classe_Original =
relevel(dados$Classe,ref = 'Boa'))
```

Armazena valores da matriz de confusao

``` r
vp <- confusao[1,1];vp
```

    ## [1] 204

``` r
fn <- confusao[2,1];fn
```

    ## [1] 37

``` r
vn <- confusao[2,2];vn
```

    ## [1] 383

``` r
fp <- confusao[1,2];fp
```

    ## [1] 75

Calculando acuracia, sensitividade e especificidade

``` r
#Calcula acuracia
acuracia <- sum(diag(confusao))/ sum(confusao);acuracia
```

    ## [1] 0.8397711

``` r
#Calcula Sensitividade
sensitividade <- vp /(vp+fn)
#Cacula Especificidade
especificidade <- vn / (vn + fp)
```

# Atividade Avaliativa do M√≥dulo 01

## Enunciado:

**Uma empresa que trabalha com aluguel de ve√≠culos deseja estudar
algumas vari√°veis que eles suspeitam que influenciam no pre√ßo do aluguel
do ve√≠culo. Para confirmar essas suspeitas, foi reunido em um conjunto
dados um hist√≥rico contendo vinte loca√ß√µes que foram exportadas
aleatoriamente do banco de dados, ou seja, temos vinte observa√ß√µes.**

Foram selecionadas sete vari√°veis para este estudo:

‚ñ™ **Pre√ßo** (vari√°vel cont√≠nua medida em reais) ‚Äì √â a vari√°vel resposta,
nos diz qual foi o pre√ßo daquela loca√ß√£o.

‚ñ™ **Portas** (vari√°vel categ√≥rica com dois n√≠veis) ‚Äì Nos informa se o
ve√≠culo alugado era de duas portas ou quatro portas.

‚ñ™ **Ar Condicionado** (vari√°vel categ√≥rica com dois n√≠veis) ‚Äì Nos
informa se o ve√≠culo alugado tinha ar-condicionado ou n√£o.

‚ñ™ **Quadrimestre** (vari√°vel categ√≥rica com tr√™s n√≠veis) ‚Äì Informa se
aquela loca√ß√£o ocorreu no primeiro, segundo ou terceiro quadrimestre do
ano.

‚ñ™ **Idade do Locat√°rio** (vari√°vel discreta medida em anos) ‚Äì Nos
informa qual a idade do indiv√≠duo que realizou a loca√ß√£o.

‚ñ™ **Quilometragem** (vari√°vel cont√≠nua medida em KM) ‚Äì Nos informa
quantos KM rodados o ve√≠culo tinha no ato da loca√ß√£o.

‚ñ™ **D√≥lar** (vari√°vel cont√≠nua medida em d√≥lares) ‚Äì Nos informa qual a
cota√ß√£o do d√≥lar no dia da loca√ß√£o.

Os alunos dever√£o desenvolver a pr√°tica e, depois, responder √†s quest√µes
objetivas propostas.

### Cria√ß√£o do Data Frame a ser utilizado no processo

Forma o conjunto de dados historico contendo vinte locacoes sorteadas
aleatoriamente do banco de dados e a armazena em um data frame chamado
dados:

``` r
dados <- data.frame(
Preco = c(368.384514890573, 446.850186825816, 
          414.72765691978, 434.291090918223, 436.652686535348, 457.65797344255, 
          490.694346597566, 474.881781399868, 458.462395897205, 412.719412673294, 
          448.799032112411, 352.040747235864, 449.461858221104, 416.150953927119, 
          416.499426750268, 551.315803331779, 462.126789471159, 515.957335395508, 
          467.598697162974, 339.548470369391), 
Portas = c("duas_portas", "quatro_portas", "duas_portas", "quatro_portas", "quatro_portas", 
          "duas_portas", "quatro_portas", "duas_portas", "quatro_portas", 
          "duas_portas", "quatro_portas", "quatro_portas", "duas_portas", 
          "quatro_portas", "duas_portas", "quatro_portas", "quatro_portas", 
          "duas_portas", "quatro_portas", "quatro_portas"),
Ar_Condicionado = c("sem_ar_condicionado",  "com_ar_condicionado", "com_ar_condicionado", "com_ar_condicionado", 
                  "com_ar_condicionado", "com_ar_condicionado", "com_ar_condicionado", 
                  "com_ar_condicionado", "com_ar_condicionado", "com_ar_condicionado", 
                  "com_ar_condicionado", "sem_ar_condicionado", "com_ar_condicionado", 
                  "com_ar_condicionado", "com_ar_condicionado", "com_ar_condicionado", 
                  "com_ar_condicionado", "com_ar_condicionado", "com_ar_condicionado", 
                  "sem_ar_condicionado"),
Quadrimestre = c("segundo_quadrimestre","segundo_quadrimestre", "segundo_quadrimestre", "segundo_quadrimestre", 
                 "segundo_quadrimestre", "terceiro_quadrimestre", "primeiro_quadrimestre", 
                 "primeiro_quadrimestre", "terceiro_quadrimestre", "segundo_quadrimestre", 
                 "terceiro_quadrimestre", "segundo_quadrimestre", "terceiro_quadrimestre", 
                 "segundo_quadrimestre", "segundo_quadrimestre", "primeiro_quadrimestre", 
                 "terceiro_quadrimestre", "primeiro_quadrimestre", "primeiro_quadrimestre", 
                 "segundo_quadrimestre"), 
Idade_Locatario = c(23, 18, 28, 21, 18, 21, 18, 20, 25, 29, 18, 33, 20, 21, 18, 21, 18, 20, 25, 29),
Quilometragem = c(957.442780544097, 829.533278217768, 923.300215829467, 871.519116905113, 930.704105677958, 554.696695914233, 501.941059782271, 
                  665.435074822519, 568.24079543466, 930.704105677958, 554.696695914233, 
                  829.533278217768, 665.435074822519, 871.519116905113, 930.704105677958, 
                  351.547138218644, 501.941059782271, 447.872006186523, 568.24079543466, 
                  930.704105677958), 
Dolar = c(4.41147933990862, 5.63014407874318, 
         8.80557934010615, 4.260591319988649, 6.93416279643155, 1.61130694543154, 
         2.57813244655973, 4.66666728709914, 1.6846066723224, 7.33872353619711, 
         4.52300814589177, 2.96689816205009, 9.91448182957733, 8.55577847959413, 
         5.93424935955983, 5.55775429484673, 6.94475470863839, 4.74330294976712, 
         4.723306965757987, 4.7010894862212))

View(dados)
```

### Pergunta 01: Explore a vari√°vel resposta PRE√áO e responda: Pelo histograma, voc√™ diria que a vari√°vel PRE√áO segue uma distribui√ß√£o normal?

1.  N√£o, pois o histograma apresenta comportamento assim√©trico a
    direita.
2.  N√£o, pois os dados s√£o distribu√≠dos simetricamente.
3.  N√£o, pois o histograma apresenta comportamento assim√©trico a
    esquerda.
4.  Sim, pela an√°lise gr√°fica a vari√°vel pre√ßo aparenta seguir uma
    distribui√ß√£o normal, pois os dados s√£o distribu√≠dos simetricamente
    em torno de um valor central.

``` r
hist(dados$Preco)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-126-1.png)<!-- -->

### Pergunta 02: Explore a vari√°vel resposta, que √© o Pre√ßo, e responda: Pelo boxplot do Pre√ßo, voc√™ consegue visualizar algum outlier?

1.  N√£o h√° outlier, pois todos os valores est√£o entre a m√©dia e dois
    desvios padr√µes.
2.  N√£o h√° nenhum outlier, pois todos os valores est√£o entre o primeiro
    e o terceiro quartil.
3.  N√£o h√° nenhum outlier, pois todos os valores s√£o pr√≥ximos √† mediana.
4.  Sim, possui um outlier superior e um inferior, ou seja, existe uma
    loca√ß√£o que o pre√ßo foi muito acima do esperado e uma loca√ß√£o cujo
    pre√ßo foi muito abaixo do esperado. Antes de remover estes outliers
    de uma an√°lise, eles devem ser investigados sobre o que ocorreu
    naquelas loca√ß√µes que fez com o que o valor fosse t√£o discrepante da
    distribui√ß√£o da vari√°vel.

``` r
boxplot(dados$Preco)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-127-1.png)<!-- -->

### Pergunta 03: Explore a vari√°vel resposta, que √© o Pre√ßo, e responda: Qual √© o valor mediano do Pre√ßo e qual a sua interpreta√ß√£o CORRETA?

1.  A mediana √© 447,8. Isso nos diz que 50% dos pre√ßos s√£o at√© este
    valor e os demais 50% s√£o acima deste valor.
2.  A mediana √© 447,8. Isso nos diz que 75% dos pre√ßos s√£o at√© este
    valor e os demais 25% s√£o acima deste valor.
3.  A mediana √© 447,8. Isso nos diz que 25% dos pre√ßos s√£o at√© este
    valor e os demais 75% s√£o acima deste valor.
4.  A mediana √© 447,8. Isso nos diz que o pre√ßo m√©dio √© 447,8

``` r
summary(dados$Preco)
```

    ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    ##   339.5   415.8   447.8   440.7   463.5   551.3

### Pergunta 04: Explore a rela√ß√£o entre as vari√°veis Pre√ßo e Quadrimestre, e responda: Atrav√©s do boxplot, como o Pre√ßo se comporta em rela√ß√£o a cada Quadrimestre?

1.  O segundo quadrimestre apresenta a maior mediana.
2.  A tr√™s medianas est√£o perfeitamente alinhadas e s√£o iguais.
3.  O primeiro quadrimestre apresenta a maior mediana.
4.  O terceiro quadrimestre apresenta a maior mediana.

``` r
boxplot(dados$Preco~ dados$Quadrimestre)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-129-1.png)<!-- -->

### Pergunta 05: Explore a rela√ß√£o entre as vari√°veis Pre√ßo e Quadrimestre, e responda: Atrav√©s de uma ANOVA, existe diferen√ßa significativa entre o Pre√ßo m√©dio de pelo menos um Quadrimestre em rela√ß√£o aos outros? Como chegou a essa conclus√£o? Adote 95% de confian√ßa na sua interpreta√ß√£o.

1.  N√£o, pois o F Value √© maior que 0,05.
2.  Sim, considerando o alfa de 0,05 e ao p valor de 0,000126, h√°
    evid√™ncias para rejeitar a hip√≥tese nula de igualdade de m√©dias, ou
    seja, pelo menos um dos quadrimestres possui o pre√ßo m√©dio diferente
    dos demais.
3.  N√£o, pois os graus de liberdade residuais deram 17, ou seja, maior
    que 0,05.
4.  N√£o, pois a m√©dia quadr√°tica dos res√≠duos √© 1014, ou seja, maior que
    0,05.

``` r
anova <- aov(Preco ~ Quadrimestre, data = dados)
summary(anova)
```

    ##              Df Sum Sq Mean Sq F value   Pr(>F)    
    ## Quadrimestre  2  32328   16164   15.95 0.000126 ***
    ## Residuals    17  17230    1014                     
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

### Pergunta 06: Explore a rela√ß√£o entre as vari√°veis Pre√ßo e Portas, e responda: Atrav√©s de um teste t de Student para amostras independentes, existe diferen√ßa significativa entre o pre√ßo m√©dio do aluguel do ve√≠culo com duas portas quando comparado com o pre√ßo m√©dio do ve√≠culo de quatro portas? Adote 95% de confian√ßa ao realizar na sua interpreta√ß√£o.

1.  Sim, pois as medianas n√£o s√£o iguais.
2.  Sim, pois os desvios padr√µes n√£o s√£o iguais.
3.  N√£o, pois ao p valor de 0,8884 n√£o h√° evid√™ncias para rejeitar a
    hip√≥tese nula de igualdade de m√©dias, ou seja, n√£o h√° diferen√ßa
    significativa entre o pre√ßo m√©dio de ve√≠culos de duas portas em
    rela√ß√£o ao pre√ßo m√©dio de ve√≠culos de quatro portas.
4.  Sim, pois o pre√ßo m√©dio dos ve√≠culos de duas portas √© 438,78 e o
    pre√ßo m√©dio dos ve√≠culos de quatro portas √© de 442,04.

``` r
boxplot(dados$Preco ~ dados$Portas)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-131-1.png)<!-- -->

``` r
#Test t de Student
t.test(dados$Preco ~ dados$Portas , 
       paired = FALSE, #amostras nao pareadas
       alternative = 'two.sided', #bilateral
       conf.level = 0.95 #95% de confianca
       )
```

    ## 
    ##  Welch Two Sample t-test
    ## 
    ## data:  dados$Preco by dados$Portas
    ## t = -0.14239, df = 17.221, p-value = 0.8884
    ## alternative hypothesis: true difference in means between group duas_portas and group quatro_portas is not equal to 0
    ## 95 percent confidence interval:
    ##  -51.48432  44.96827
    ## sample estimates:
    ##   mean in group duas_portas mean in group quatro_portas 
    ##                    438.7862                    442.0443

### Pergunta 07: Explore a rela√ß√£o entre as vari√°veis Pre√ßo e Quilometragem, e responda: Pelo gr√°fico de dispers√£o, voc√™ identifica que existe rela√ß√£o linear entre o Pre√ßo e a Quilometragem? Se sim, a rela√ß√£o √© positiva ou negativa?

1.  Existe rela√ß√£o, mas √© uma rela√ß√£o quadr√°tica.
2.  Sim, a rela√ß√£o √© linear negativa, pois √† medida que a quilometragem
    aumenta o pre√ßo diminui.
3.  N√£o existe rela√ß√£o entre as duas vari√°veis.
4.  Sim, a rela√ß√£o √© positiva.

``` r
plot(y = dados$Preco ,
     x = dados$Quilometragem,
     pch = 16)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-133-1.png)<!-- -->

### Pergunta 08: Explore a rela√ß√£o entre as vari√°veis Pre√ßo e Quilometragem, e responda: Obtenha o valor do coeficiente de correla√ß√£o linear de Pearson entre o Pre√ßo e a Quilometragem. Qual a interpreta√ß√£o CORRETA?

1.  O coeficiente de correla√ß√£o linear de Pearson √© -0,82, isso nos
    informa que √© uma correla√ß√£o negativa alta.
2.  O coeficiente de correla√ß√£o linear de Pearson √© -0,82, isso nos
    informa que √© uma correla√ß√£o negativa baixa.
3.  O coeficiente de correla√ß√£o linear de Pearson √© -0,82, isso nos
    informa que √© uma correla√ß√£o positiva alta.
4.  O coeficiente de correla√ß√£o linear de Pearson √© -0,82, isso nos
    informa que n√£o h√° correla√ß√£o.

``` r
cor(dados$Preco, dados$Quilometragem)
```

    ## [1] -0.8231561

### Pergunta 09: Explore a rela√ß√£o entre as vari√°veis Pre√ßo e Quilometragem, e responda: Se tentarmos utilizar somente a Quilometragem para prever o valor do Pre√ßo, o quanto da varia√ß√£o do Pre√ßo a vari√°vel Quilometragem consegue explicar? Em outas palavras, interprete o R2 da regress√£o linear do Pre√ßo em fun√ß√£o da Quilometragem.

1.  O R2 √© de 67,76%, ou seja, a vari√°vel Quilometragem consegue
    explicar 67,76% da varia√ß√£o do Pre√ßo.
2.  O R2 √© de 67,76%, ou seja, para cada quilometro aumentado, o Pre√ßo
    aumenta em m√©dia 67,76%.
3.  O R2 √© de 67,76%, ou seja, para cada quilometro aumentado, o Pre√ßo
    diminui em m√©dia 67,76%.
4.  O R2 √© de 67,76%, ou seja, para cada quilometro aumentado, o Pre√ßo
    aumenta em m√©dia 32,24% (1-0,6776).

``` r
regressao_linear <- lm(Preco ~ Quilometragem, data = dados)
summary(regressao_linear)
```

    ## 
    ## Call:
    ## lm(formula = Preco ~ Quilometragem, data = dados)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -65.477 -18.862   5.824  20.919  40.446 
    ## 
    ## Coefficients:
    ##                Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)   592.25689   25.51961  23.208 7.28e-15 ***
    ## Quilometragem  -0.21065    0.03425  -6.151 8.29e-06 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 29.79 on 18 degrees of freedom
    ## Multiple R-squared:  0.6776, Adjusted R-squared:  0.6597 
    ## F-statistic: 37.83 on 1 and 18 DF,  p-value: 8.293e-06

### Pergunta 10: Explore a vari√°vel Quilometragem, e responda: Qual o valor do primeiro quartil e qual a sua interpreta√ß√£o CORRETA?

1.  O primeiro quartil √© 747,5, isso nos diz que at√© 25% dos ve√≠culos
    alugados possuem quilometragem at√© 554,7.
2.  O primeiro quartil √© 554,7, isso nos diz que at√© 50% dos ve√≠culos
    alugados possuem quilometragem at√© 554,7.
3.  O primeiro quartil √© 554,7, isso nos diz que at√© 75% dos ve√≠culos
    alugados possuem quilometragem at√© 554,7.
4.  O primeiro quartil √© 554,7, isso nos diz que at√© 25% dos ve√≠culos
    alugados possuem quilometragem at√© 554,7.

``` r
summary(dados$Quilometragem)
```

    ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    ##   351.5   554.7   747.5   719.3   925.2   957.4

### Pergunta 11: Explore a vari√°vel Quilometragem, e responda: Qual o valor do terceiro quartil e qual a sua interpreta√ß√£o CORRETA?

1.  O terceiro quartil √© 925,2, isso nos diz que at√© 50% dos ve√≠culos
    alugados possuem quilometragem at√© 925,2.
2.  O terceiro quartil √© 925,2, isso nos diz que at√© 25% dos ve√≠culos
    alugados possuem quilometragem at√© 925,2.
3.  O terceiro quartil √© 925,2, isso nos diz que at√© 75% dos ve√≠culos
    alugados possuem quilometragem at√© 925,2.
4.  O terceiro quartil √© 925,2, isso nos diz que at√© 75% dos ve√≠culos
    alugados possuem quilometragem at√© 719,3.

``` r
summary(dados$Quilometragem)
```

    ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    ##   351.5   554.7   747.5   719.3   925.2   957.4

### Pergunta 12: Explore a vari√°vel Quilometragem e responda: Qual √© o valor do coeficiente de varia√ß√£o e qual a sua interpreta√ß√£o CORRETA?

1.  O coeficiente de varia√ß√£o √© 27,74%, ou seja, os valores da vari√°vel
    quilometragem variam em m√©dia 27,74% em torno de sua m√©dia.
2.  O coeficiente de varia√ß√£o √© 27,74%, ou seja, os valores da vari√°vel
    quilometragem variam em m√©dia 27,74% em torno do terceiro quartil.
3.  O coeficiente de varia√ß√£o √© 27,74%, ou seja, os valores da vari√°vel
    quilometragem variam em m√©dia 27,74% em torno do desvio padr√£o.
4.  O coeficiente de varia√ß√£o √© 27,74%, ou seja, os valores da vari√°vel
    quilometragem variam em m√©dia 27,74% em torno do primeiro quartil.

``` r
sd(dados$Quilometragem) / mean(dados$Quilometragem)
```

    ## [1] 0.2774628

### Pergunta 13: Explore a correla√ß√£o entre o Dolar e o Preco, e responda: A correla√ß√£o entre as duas vari√°veis √© positiva ou negativa?

1.  A correla√ß√£o √© negativa.
2.  A correla√ß√£o √© positiva.
3.  Existe uma correla√ß√£o c√∫bica.
4.  O gr√°fico de dispers√£o n√£o apresenta nenhum padr√£o entre as duas
    vari√°veis, ou seja, na medida que o D√≥lar aumenta, o Pre√ßo n√£o
    cresce nem decresce, ou seja, n√£o h√° correla√ß√£o entre as duas
    vari√°veis.

``` r
plot(y = dados$Preco,
     x = dados$Dolar,
     pch = 16)
```

![](Apostila---Estatistica_files/figure-gfm/unnamed-chunk-139-1.png)<!-- -->

### Pergunta 14: Explore a correla√ß√£o entre o Dolar e o Preco, e responda: Obtenha o coeficiente de correla√ß√£o linear de Pearson entre o Dolar e o Pre√ßo. Qual a interpreta√ß√£o CORRETA?

1.  O valor do coeficiente de correla√ß√£o linear de Pearson √© -0,06, o
    que indica correla√ß√£o positiva fraca.
2.  O valor do coeficiente de correla√ß√£o linear de Pearson √© -0,06, o
    que indica correla√ß√£o negativa forte.
3.  O valor do coeficiente de correla√ß√£o linear de Pearson √© -0,06, que
    indica aus√™ncia de correla√ß√£o linear.
4.  O valor do coeficiente de correla√ß√£o linear de Pearson √© -0,06, o
    que indica correla√ß√£o negativa moderada.

``` r
cor(dados$Preco, dados$Dolar)
```

    ## [1] -0.06982716

### Pergunta 15: Explore a correla√ß√£o entre o Dolar e o Preco, e responda: Se ajustarmos uma regress√£o linear entre o Dolar e o Preco, para tentar prever o Pre√ßo baseado no D√≥lar, seria poss√≠vel?

1.  Seria poss√≠vel, pois a estat√≠stica F √© menor que 5%.
2.  Seria poss√≠vel, pois o p valor do coeficiente beta do Dolar √© acima
    de 5%.
3.  Seria poss√≠vel, pois o valor do coeficiente beta √© negativo -1,573.
4.  N√£o seria poss√≠vel, pois o p valor do coeficiente beta do Dolar √© de
    0,77 (77%), ou seja, independente do n√≠vel de signific√¢ncia adotado,
    a vari√°vel Dolar n√£o exerce influ√™ncia significativa na vari√°vel
    Preco, portanto, n√£o √© poss√≠vel prever o Pre√ßo baseado no D√≥lar.

``` r
regressao_linear <- lm(Preco ~ Dolar, data = dados)
summary(regressao_linear)
```

    ## 
    ## Call:
    ## lm(formula = Preco ~ Dolar, data = dados)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -102.173  -21.224    6.694   24.429  110.942 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  449.115     30.530  14.711 1.78e-11 ***
    ## Dolar         -1.573      5.296  -0.297     0.77    
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 52.34 on 18 degrees of freedom
    ## Multiple R-squared:  0.004876,   Adjusted R-squared:  -0.05041 
    ## F-statistic: 0.08819 on 1 and 18 DF,  p-value: 0.7699
